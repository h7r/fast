{
  
    
        "post0": {
            "title": "Title",
            "content": "Table of Contents . Model 1 : flop cards | Playing with threshold | . | Features importance visualization | Model 2 : adding features | Model 3 : rank 1, rank 2 and rank 3 | Building the isomorphic 1755 flops | . from pathlib import Path from pokerange import Range from scipy.stats import linregress import re import pandas as pd import numpy as np import matplotlib.pyplot as plt %pylab inline plt.style.use(&#39;fivethirtyeight&#39;) . Populating the interactive namespace from numpy and matplotlib . GTO solvers like PioSolver, GTO+, Monkey, SimplePostflop compute complex equilibriums strategies that are not necessary understandable not even less implementable by humans since they have computational limitations. Due to this limitations, according to Kevin Burns (&quot;Style in Poker&quot;, 2006), humans generally &quot;settle for a good enough or satisficing strategy&quot;. In other words: . a reduced rule-set or heuristic that simplifies a player’s mental efforts . So, we are interested here after Ganzfried &amp; Yusuf (2017) for example, in computing strategies thant can be implementred by humans. How we do that? . We precomputed mathematical or optimal strategies with GTO+ in database mode (1755 flops, the isomorphic ones). It took us 7 hours with a reduced tree:Hero at the BB and Villain at the EP can only bet the geometric growth of the pot (GGOP) (Chen &amp; Ankenman). The optimal bet size $B$ in percentage of the pot for 3 streets to come is calculated as following: begin{equation*} B = frac{1}{2}(GGOP-1) end{equation*} | begin{equation*} GGOP = sqrt[ leftroot{-1} uproot{2} scriptstyle 3]{ frac{(2S + P)}{P}} end{equation*}where $P$ is the starting pot amount and $S$ the effective stacks. Since $P = 20.5$ on the flop and $S = 90$, $B=0,5693$. . (By applying LogisticRegression technique to a set of 1755 flops (the isomorphic ones) for which we first compute the optimal strategies with GTO+ software, we will try to create such a reduced rule-set so that humans can apply it during real g. . import math #geometric growth of the pot or pot growth rate ggop=((90*2+20.5)/20.5)**(1/3) log(200.5/20.5)/log(ggop) . 3.0 . 90*2+20.5 . 200.5 . b=(ggop - 1)*0.5 b . 0.5692768793395864 . P = 20.5 S=90 for street in range(0,3): B=b*P P+=(2*B) S-=B print(B, P, S) . 11.67017602646152 43.84035205292304 78.32982397353848 24.957298805836857 93.75494966459675 53.372525167701625 53.37252516770164 200.50000000000003 -1.4210854715202004e-14 . P+(0.6463414634146342*P)*2 . 459.68292682926835 . S-(0.6463414634146342*P) . -129.59146341463418 . ((1170.386634834831 - 20.5)/2)**1/3 . 191.64777247247184 . all_cards = [&#39;2h&#39;, &#39;3h&#39;, &#39;4h&#39;, &#39;5h&#39;, &#39;6h&#39;, &#39;7h&#39;, &#39;8h&#39;, &#39;9h&#39;, &#39;Th&#39;, &#39;Jh&#39;, &#39;Qh&#39;, &#39;Kh&#39;, &#39;Ah&#39;, &#39;2d&#39;, &#39;3d&#39;, &#39;4d&#39;, &#39;5d&#39;, &#39;6d&#39;, &#39;7d&#39;, &#39;8d&#39;, &#39;9d&#39;, &#39;Td&#39;, &#39;Jd&#39;, &#39;Qd&#39;, &#39;Kd&#39;, &#39;Ad&#39;, &#39;2c&#39;, &#39;3c&#39;, &#39;4c&#39;, &#39;5c&#39;, &#39;6c&#39;, &#39;7c&#39;, &#39;8c&#39;, &#39;9c&#39;, &#39;Tc&#39;, &#39;Jc&#39;, &#39;Qc&#39;, &#39;Kc&#39;, &#39;Ac&#39;, &#39;2s&#39;, &#39;3s&#39;, &#39;4s&#39;, &#39;5s&#39;, &#39;6s&#39;, &#39;7s&#39;, &#39;8s&#39;, &#39;9s&#39;, &#39;Ts&#39;, &#39;Js&#39;, &#39;Qs&#39;, &#39;Ks&#39;, &#39;As&#39;] ranks = [&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;T&#39;,&#39;J&#39;,&#39;Q&#39;,&#39;K&#39;,&#39;A&#39;] suits = [&quot;h&quot;, &quot;d&quot;, &quot;c&quot;,&quot;s&quot;] DATA_DIR = Path.cwd().parent / &#39;data&#39; . bb_str = open( DATA_DIR / &quot;01_pk_bb_3br_2020-10-20.txt&quot;,&quot;r&quot;).read() ep_str = open( DATA_DIR / &quot;02_pk_ep_call3br_2020-10-20.txt&quot;,&quot;r&quot;).read() . bb = Range(bb_str, &quot;BB 3bet range&quot;) ep = Range(ep_str, &quot;EP call to 3bet range&quot;) . bb.displayRange() . ep.displayRange() . # return rank number [range(0,13)] def getRank(card): return(card%13) # input : card between 0 and 51 # return suit number [0,1,2,3] def getSuit(card): return(card//13) def cards_input(cards = None): matrix = np.zeros(shape = (4,13), dtype = np.float64) if cards is None: return matrix for i in cards: r = getRank(i) s = getSuit(i) matrix[s][r] = 1.0 return matrix crds=cards_input([51,50]) print(crds) . [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]] . . # flops = pd.read_csv(&quot;data/csv/player2.csv&quot;,delimiter=&quot; t&quot;,header=0, index_col=False) # flops=flops.set_index(&#39;flop&#39;).sort_index() # flops = pd.read_csv(&quot;data/csv/_pk_tanagra_output.csv&quot;, delimiter=&quot;,&quot;, header=0, index_col=[0]).sort_index() # #flops.to_csv(&quot;data/csv/_pk_tanagra_output.csv&quot;,index=False) # usecols=range(1,8) # flops = pd.read_csv(&quot;data/csv/01_gto_3bp_player1_bb-vs-btn.csv&quot;, # delimiter=&quot; t&quot;, header=0, index_col=[0],usecols=usecols).sort_index() # col_names=[&quot;bet75&quot;,&quot;bet50&quot;,&quot;bet33&quot;] # to_rename=dict() # for idx, col in enumerate(flops.columns[2:5]): # to_rename[col]=col_names[idx] # flops=flops.rename(columns=to_rename) . cols = [&quot;flops&quot;,&quot;equity&quot;,&quot;ev&quot;,&quot;bet&quot;,&quot;check&quot;] #df = pd.read_csv( DATA_DIR / &quot;03_pk_bb-vs-ep_3BP_2020-10-21.txt&quot;,delimiter=&quot; t&quot;, names=cols, index_col = False) df = pd.read_csv( DATA_DIR / &quot;04_pk_bb-vs-ep_3BP_2020-11-11.txt&quot;,delimiter=&quot; t&quot;, names=cols, index_col = False) flops = df.set_index(&quot;flops&quot;).sort_index() . flops.head() . equity ev bet check . flops . 2d2c2h | 48.214 | 10.986 | 56.517 | 43.483 | . 3c2d2c | 47.744 | 9.939 | 56.109 | 43.891 | . 3d3c2c | 48.334 | 10.447 | 61.874 | 38.126 | . 3d3c3h | 49.076 | 11.543 | 60.937 | 39.063 | . 3d3h2c | 48.017 | 10.622 | 58.074 | 41.926 | . P = 20.5 fig = plt.figure(figsize = (15,10)) ev = [i/100*(P) for i in range(0,100)] #ev = [i/100*(P - P*0.05) if (P*0.05&lt;2.5) else i/100*(P - 2.5) for i in range(0,100)] plt.xlim(45,60) plt.xlabel(&quot;Raw equity&quot;) plt.ylabel(&quot;EV&quot;) plt.ylim(7,14) plt.plot(range(0,100),ev, &quot;g--&quot;, c =&quot;c&quot;, label=&quot;EV&quot;) plt.scatter(flops.equity, flops.ev, c=&quot;orange&quot;) plt.legend() plt.title(&quot;Player 1 - Entire range - 1755 flops&quot;) #since starting pot = 20.5 we can calculate the ev line before rake #(amount in bb player 1 should win if he was realizing 100% of his equity) #ev = [(i - i*0.025) if (i*0.025&lt;2.5) else (i - 2.5) for i in ev] . Text(0.5, 1.0, &#39;Player 1 - Entire range - 1755 flops&#39;) . The figure above shows that some flops, those above the EV line, have a capture factor we call $R$ superior to 1. That means Player 1 can expect to capture in average on later streets a fraction of the pot $P$ such as $EV &gt; E * P$ by a factor equal to $R$. . $EV = E * P * R$ . Most of the time Player 1 will not be able to realize 100% of his equity (point belows the dot line), though most of flops give him a range advantage. At first glance, there are three groups of flops, less populated from left to right but we are not interested in clustering them by this way. . As said by Will Tipton: . That is, $R$ is not only a function of equity. It also depends on things like playability of all the hands in Villain’s range. . In other terms, when $R &gt; 1$, Player&#39;s 1 range has a better playability than Villain&#39;s range. The purpose of his article is to find, using machine learning techniques, the flop characteristics which are likely to give a better playability to Player 1&#39;s range and resume them into simple heuristics easy to apply by a player at a poker table. . fig = plt.figure(figsize = (15,5)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122,sharex=ax1) y = flops.ev ax1.scatter(flops.bet, y) ax1.set_title(&quot;Bet frequency vs EV&quot;) ax1.set_ylabel(&quot;ev&quot;) ax1.set_xlabel(&quot;bet&quot;) ax1.set_ylim(7,14) #ax1.grid(True) bet_regress = linregress(flops.bet, y) ax1.plot(flops.bet, flops.bet*bet_regress[0]+bet_regress[1], c = &quot;y&quot;) print(&quot;R-squared: %f&quot; % bet_regress[2]**2) y = flops.equity ax2.scatter(flops.bet, y) ax2.set_title(&quot;Bet frequency vs Equity&quot;) ax2.set_ylabel(&quot;equity&quot;) ax2.set_xlabel(&quot;bet&quot;) ax2.set_ylim(45,60) #ax2.grid(True) bet_regress = linregress(flops.bet, y) ax2.plot(flops.bet, flops.bet*bet_regress[0]+bet_regress[1], c = &quot;y&quot;) print(&quot;R-squared: %f&quot; % bet_regress[2]**2) . R-squared: 0.155819 R-squared: 0.000633 . There is clearly a positive correlation between betting frequency and EV, while raw equity on the flop doesn&#39;t seem to impact player 1&#39;s betting strategy. . Since we search for simple heuristics that an human could memorize and use while playing, we will build our predictive features using only public information shared by both players: the three-cards flop. . Let&#39;s now separate flops which allow to realize more than 100% of equity from others. . flops_above_ev = [1 if ev&gt;flops.equity[idx]/100*(P) else 0 for idx, ev in enumerate(flops.ev)] flops[&quot;above_ev&quot;]= flops_above_ev flops[&quot;above_ev&quot;] . flops 2d2c2h 1 3c2d2c 1 3d3c2c 1 3d3c3h 1 3d3h2c 1 .. Th9d5c 0 Th9d6c 0 Th9d7c 0 Th9d8c 0 Th9d9c 0 Name: above_ev, Length: 1755, dtype: int64 . fig = plt.figure(figsize = (15,5)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122,sharex=ax1) y = flops.ev[flops[&quot;above_ev&quot;]==1] x = flops.bet[flops[&quot;above_ev&quot;]==1] ax1.scatter(x, y) ax1.set_title(&quot;Bet frequency when R &gt; 1&quot;) ax1.set_ylabel(&quot;ev&quot;) ax1.set_xlabel(&quot;bet&quot;) ax1.set_ylim(7,14) #ax1.grid(True) bet_regress = linregress(x, y) ax1.plot(x, x*bet_regress[0]+bet_regress[1], c = &quot;y&quot;) print(&quot;R-squared: %f&quot; % bet_regress[2]**2) y = flops.ev[flops[&quot;above_ev&quot;]==0] x = flops.bet[flops[&quot;above_ev&quot;]==0] ax2.scatter(x, y) ax2.set_title(&quot;Bet frequency when R &lt;= 1&quot;) ax2.set_ylabel(&quot;ev&quot;) ax2.set_xlabel(&quot;bet&quot;) ax2.set_ylim(7,14) #ax2.grid(True) bet_regress = linregress(x, y) ax2.plot(x, x*bet_regress[0]+bet_regress[1], c = &quot;y&quot;) print(&quot;R-squared: %f&quot; % bet_regress[2]**2) . R-squared: 0.128352 R-squared: 0.002253 . Pretty clear what happens here! It makes sense since $R$ cannot be superior to $1$ when both players are checking the two remaining streets before going to showdown. In this case, winning hands will have a $R$ equal to $1$ since the amount of the pot at the showdown is the same than on the flop. The only way to have $R&gt;1$ is to bet at least one time and to be called at least one time. This is why Player&#39;s is more likely to play more aggressively when the flop is such that it gives a better playability to his range. . fig = plt.figure(figsize = (15,10)) y1 = flops.ev[flops[&quot;above_ev&quot;]==1] x1 = flops.bet[flops[&quot;above_ev&quot;]==1] plt.scatter(x1, y1) y2 = flops.ev[flops[&quot;above_ev&quot;]==0] x2 = flops.bet[flops[&quot;above_ev&quot;]==0] plt.scatter(x2, y2) plt.xlabel(&quot;bet frequency&quot;) plt.ylabel(&quot;ev&quot;) . Text(0, 0.5, &#39;ev&#39;) . Model 1 : flop cards . x = np.zeros(shape = (len(flops),52), dtype=np.float32) for flop in range(0, len(flops)): idx1,idx2,idx3 = flops.index[flop][0:2],flops.index[flop][2:4],flops.index[flop][4:6] #crds=list() for idx, card in enumerate(all_cards): #print card if (idx1 == card) or (idx2 == card) or (idx3 == card): x[flop][idx]+=1 . print(set([i for i in np.sum(x,axis=1)])) . {3.0} . # y = np.zeros(shape = (len(flops)), dtype=np.float32) # #mean = flops.ev.mean() # for idx, i in enumerate(flops.ev): # if i &gt; mean: # y[idx]+=1 y = flops.above_ev . np.shape(x)[0] == np.shape(y)[0] . True . from sklearn.model_selection import StratifiedKFold from sklearn.linear_model import LogisticRegressionCV from sklearn.metrics import confusion_matrix, classification_report from sklearn.metrics import precision_recall_curve, recall_score, precision_score from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score from sklearn.metrics import f1_score from sklearn.metrics import auc from sklearn.metrics import accuracy_score from sklearn.metrics import hamming_loss #from sklearn.model_selection import train_test_split #from sklearn.preprocessing import StandardScaler . %%time w = [{0:1000,1:100},{0:1000,1:10}, {0:1000,1:1.0}, {0:500,1:1.0}, {0:400,1:1.0}, {0:300,1:1.0}, {0:200,1:1.0}, {0:150,1:1.0}, {0:100,1:1.0}, {0:99,1:1.0}, {0:10,1:1.0}, {0:0.01,1:1.0}, {0:0.01,1:10}, {0:0.01,1:100}, {0:0.001,1:1.0}, {0:0.005,1:1.0}, {0:1.0,1:1.0}, {0:1.0,1:0.1}, {0:10,1:0.1}, {0:100,1:0.1}, {0:10,1:0.01}, {0:1.0,1:0.01}, {0:1.0,1:0.001}, {0:1.0,1:0.005}, {0:1.0,1:10}, {0:1.0,1:99}, {0:1.0,1:100}, {0:1.0,1:150}, {0:1.0,1:200}, {0:1.0,1:300},{0:1.0,1:400},{0:1.0,1:500}, {0:1.0,1:1000}, {0:10,1:1000},{0:100,1:1000} ] crange = np.arange(0.5, 20.0, 0.5) skf = StratifiedKFold(n_splits=5) model = LogisticRegressionCV(Cs=crange,random_state=13, cv = skf, scoring = &quot;f1&quot;, class_weight=w).fit(x,y) . Wall time: 2.12 s . precision, recall, thresholds = precision_recall_curve(y, model.predict_proba(x)[:,1]) print(precision) print(recall) print(thresholds) . [0.2252042 0.2247519 0.22488318 ... 1. 1. 1. ] [1. 0.99740933 0.99740933 ... 0.00518135 0.00259067 0. ] [2.34626279e-04 2.39435941e-04 2.49108690e-04 ... 9.95949710e-01 9.96274724e-01 9.99658622e-01] . model.predict_proba(x)[:,1] . array([0.83624981, 0.27787832, 0.26286225, ..., 0.11391788, 0.08916897, 0.07737925]) . plt.plot(thresholds, precision[1:], marker=&#39;.&#39;, label=&#39;Precision&#39;) plt.plot(thresholds, recall[1:], marker=&#39;.&#39;, label=&#39;Recall&#39;) no_skill = len(y[y==1]) / len(y) plt.plot([0, 1], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;) # axis labels plt.xlabel(&#39;Threshold&#39;) #pyplot.ylabel(&#39;Precision&#39;) # show the legend plt.legend() # show the plot plt.show() . Playing with threshold . p = model.predict_proba(x) # keep probabilities for the positive outcome only p = p[:, 1] y_threshold = [1 if i&gt;0.40 else 0 for i in p] print(classification_report(y, y_threshold)) . precision recall f1-score support 0 0.91 0.93 0.92 1369 1 0.74 0.69 0.71 386 accuracy 0.88 1755 macro avg 0.83 0.81 0.82 1755 weighted avg 0.88 0.88 0.88 1755 . classification_report(y, y_threshold,output_dict=True)[&quot;1&quot;] . {&#39;precision&#39;: 0.7375690607734806, &#39;recall&#39;: 0.6917098445595855, &#39;f1-score&#39;: 0.7139037433155081, &#39;support&#39;: 386} . print(classification_report(y, model.predict(x))) . precision recall f1-score support 0 0.98 0.98 0.98 1369 1 0.91 0.91 0.91 386 accuracy 0.96 1755 macro avg 0.94 0.94 0.94 1755 weighted avg 0.96 0.96 0.96 1755 . cm = confusion_matrix(y, model.predict(x)) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm, cmap=&quot;Blues_r&quot;, interpolation=&quot;nearest&quot;) ax.grid(False) ax.set_xlabel(&#39;Predicted outputs&#39;, color=&#39;black&#39;) ax.set_ylabel(&#39;Actual outputs&#39;, color=&#39;black&#39;) ax.xaxis.set(ticks=range(2)) ax.yaxis.set(ticks=range(2)) #ax.set_ylim(2, -1) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;black&#39;) plt.show() print(f&#39;Total positives: {np.sum(model.decision_function(x)&gt;0)}&#39;) . Total positives: 353 . ns_probs = [0 for _ in range(len(y))] # predict probabilities lr_probs = model.predict_proba(x) # keep probabilities for the positive outcome only lr_probs = lr_probs[:, 1] # calculate scores ns_auc = roc_auc_score(y, ns_probs) lr_auc = roc_auc_score(y, lr_probs) # summarize scores print(&#39;No Skill: ROC AUC=%.3f&#39; % (ns_auc)) print(&#39;Logistic: ROC AUC=%.3f&#39; % (lr_auc)) # calculate roc curves ns_fpr, ns_tpr, _ = roc_curve(y, ns_probs) lr_fpr, lr_tpr, _ = roc_curve(y, lr_probs) # plot the roc curve for the model plt.plot(ns_fpr, ns_tpr, linestyle=&#39;--&#39;, label=&#39;No Skill&#39;) plt.plot(lr_fpr, lr_tpr, marker=&#39;.&#39;, label=&#39;Logistic&#39;) # axis labels plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) # show the legend plt.legend() # show the plot plt.show() . No Skill: ROC AUC=0.500 Logistic: ROC AUC=0.988 . yhat = model.predict(x) # predict probabilities lr_probs = model.predict_proba(x) # keep probabilities for the positive outcome only lr_probs = lr_probs[:, 1] lr_precision, lr_recall, _ = precision_recall_curve(y, lr_probs) lr_f1, lr_auc = f1_score(y, yhat), auc(lr_recall, lr_precision) # summarize scores print(&#39;Logistic: f1=%.3f auc=%.3f&#39; % (lr_f1, lr_auc)) # plot the precision-recall curves no_skill = len(y[y==1]) / len(y) plt.plot([0, 1], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;) plt.plot(lr_recall, lr_precision, marker=&#39;.&#39;, label=&#39;Logistic&#39;) # axis labels plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) # show the legend plt.legend() # show the plot plt.show() . Logistic: f1=0.912 auc=0.950 . flops[&quot;p_0&quot;]=model.predict_proba(x)[:,0] flops[&quot;p_1&quot;]=model.predict_proba(x)[:,1] flops[&quot;true_p&quot;]=y . flops[flops.above_ev==1].sort_values(&quot;ev&quot;,ascending=False).head() . equity ev bet check above_ev p_0 p_1 true_p . flops . Kh3d3c | 54.812 | 13.837 | 93.686 | 6.314 | 1 | 0.055136 | 0.944864 | 1 | . AcKdKc | 58.425 | 13.752 | 77.292 | 22.708 | 1 | 0.006368 | 0.993632 | 1 | . Kh4d4c | 55.328 | 13.691 | 96.967 | 3.033 | 1 | 0.045363 | 0.954637 | 1 | . Kh6d6c | 55.520 | 13.549 | 98.330 | 1.670 | 1 | 0.075271 | 0.924729 | 1 | . Kh7d7c | 55.761 | 13.449 | 97.952 | 2.048 | 1 | 0.171977 | 0.828023 | 1 | . from sklearn.utils.class_weight import compute_class_weight weighting = compute_class_weight(&#39;balanced&#39;, [0,1], y) print(weighting) . [0.64097882 2.27331606] . P = 20.5 fig = plt.figure(figsize = (15,10)) ev = [i/100*(P) for i in range(0,100)] #ev = [i/100*(P - P*0.05) if (P*0.05&lt;2.5) else i/100*(P - 2.5) for i in range(0,100)] plt.xlim(45,60) plt.xlabel(&quot;Equity&quot;) plt.ylabel(&quot;EV&quot;) plt.ylim(7,14) plt.plot(range(0,100),ev, &quot;g--&quot;, c =&quot;c&quot;, label=&quot;EV&quot;) false_negatives = (flops[&quot;p_1&quot;]&lt;=0.5) &amp; (flops[&quot;true_p&quot;]==1) false_positives = (flops[&quot;p_1&quot;]&gt;0.5) &amp; (flops[&quot;true_p&quot;]==0) true_positives = (flops[&quot;p_1&quot;]&gt;0.5) &amp; (flops[&quot;true_p&quot;]==1) true_negatives = (flops[&quot;p_1&quot;]&lt;=0.5) &amp; (flops[&quot;true_p&quot;]==0) plt.scatter(flops.equity[false_negatives], flops.ev[false_negatives], label=&quot;false_negatives&quot;) plt.scatter(flops.equity[false_positives], flops.ev[false_positives], label=&quot;false_positives&quot;) plt.scatter(flops.equity[true_positives], flops.ev[true_positives], label=&quot;true_positives&quot;) plt.scatter(flops.equity[true_negatives], flops.ev[true_negatives], label=&quot;true_negatives&quot;) plt.legend(loc=&quot;upper left&quot;) plt.title(&quot;Player 1 - Entire range - 1755 flops&quot;) . Text(0.5, 1.0, &#39;Player 1 - Entire range - 1755 flops&#39;) . fig = plt.figure(figsize = (15,10)) y1 = flops.ev[false_negatives] y2 = flops.ev[false_positives] y3 = flops.ev[true_positives] y4 = flops.ev[true_negatives] x1 = flops.bet[false_negatives] x2 = flops.bet[false_positives] x3 = flops.bet[true_positives] x4 = flops.bet[true_negatives] plt.scatter(x1, y1, label=&quot;false_negatives&quot;) plt.scatter(x2, y2, label=&quot;false_positives&quot;) plt.scatter(x3, y3, label=&quot;true_positives&quot;) plt.scatter(x4, y4, label=&quot;true_negatives&quot;) plt.legend(loc=&quot;upper left&quot;) plt.xlabel(&quot;bet frequency&quot;) plt.ylabel(&quot;ev&quot;) . Text(0, 0.5, &#39;ev&#39;) . # from sklearn.model_selection import GridSearchCV # # define hyperparameters # hyperparam_grid = {&quot;class_weight&quot;: w # ,&quot;penalty&quot;: [&quot;l1&quot;, &quot;l2&quot;] # ,&quot;C&quot;: crange # ,&quot;fit_intercept&quot;: [True, False] } # # logistic model classifier # lg4 = LogisticRegression(random_state=13)# define evaluation procedure # grid = GridSearchCV(lg4,hyperparam_grid,scoring=&quot;precision&quot;, cv=100, n_jobs=-1, refit=True) # grid.fit(x,y) # print(f&#39;Best score: {grid.best_score_} with param: {grid.best_params_}&#39;) . close = np.isclose(model.predict(x),y) np.sum(close)/1755 . 0.89002849002849 . Features importance visualization . np.std(x, 0)*model.coef_.ravel() . array([ 0.0313695 , 0.08458638, 0.06636436, 0.15067103, 0.03736676, 0.02296171, 0.01006303, -0.03836432, -0.18336434, -0.23581481, -0.12542221, 0.49774583, -0.01374151, 0.48488632, 0.49384785, 0.47108331, 0.29035467, 0.39606533, 0.3270813 , 0.3104842 , 0.31478304, -0.25562584, -0.49422812, -0.02051507, 1.04763126, 0.38563581, -0.55778464, -0.47849127, -0.32973117, -0.43116665, -0.37515618, -0.60088619, -0.68832152, -0.73360519, -1.42855254, -1.56362575, -0.79909028, 0.50248598, -0.1721374 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) . c = np.std(x, 0)*model.coef_.ravel() coefs = np.zeros(shape=(1,len(x.columns))) for idx, i in enumerate(c): coefs[0][idx]+=i np.shape(model.coef_) if np.shape(x)[1]&gt;52: coef2_ = coefs[:,52:] coef1_ = coefs[:,0:52] elif np.shape(x)[1]&lt;52: coef1_ = None coef2_ = coefs else: coef2_ = None coef1_ = coefs . if coef1_ is not None: coeffs_by_rank = np.zeros(shape=(1,13), dtype = np.float64) for i in range(0,52): j= getRank(i) val = coef1_[0][i] coeffs_by_rank[0][j]+=val coeffs_by_rank = coeffs_by_rank / 4 else: pass . plt.style.use(&#39;fivethirtyeight&#39;) display(&quot;Score: &quot; + str(round(model.score(x, y),2))) if coef2_ is not None: fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(0,len(x.columns[52:62])), x.columns[52:62]) plt.yticks([]) plt.imshow(coef2_[:,:10],interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(),coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.grid(False) if coef1_ is not None: fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(13), ranks) plt.yticks([]) plt.imshow(coeffs_by_rank,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(), coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.grid(False) if coef1_ is not None: coefs_=np.zeros(shape=(4,13), dtype = np.float64) for i in range(0,52): rank = getRank(i) suit = getSuit(i) val = coef1_ [0][i] coefs_[suit][rank]+=val plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.yticks(range(4), suits) plt.xticks(range(13), ranks) img = ax.imshow(coefs_,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(), coefs.max(), 5) plt.colorbar(img, shrink=.3, ticks=bins) plt.grid(False) plt.show() . C: Users Wilfrid Anaconda3 lib site-packages sklearn linear_model logistic.py:2260: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22. ChangedBehaviorWarning) . &#39;Score: 0.91&#39; . pd.DataFrame(coeffs_by_rank.ravel(),index=ranks,columns=[&quot;coef&quot;]).sort_values(&quot;coef&quot;,ascending=False) . coef . K | 0.342059 | . 4 | 0.068356 | . 6 | 0.039422 | . 3 | 0.036880 | . 5 | 0.024343 | . 2 | 0.007836 | . A | -0.000165 | . 7 | -0.041298 | . 9 | -0.059532 | . 8 | -0.066771 | . Q | -0.067043 | . T | -0.171650 | . J | -0.219868 | . ranks_df = pd.DataFrame(coeffs_by_rank.ravel(),index=ranks,columns=[&quot;coef&quot;]) features_df = pd.DataFrame(coef2_.ravel(),index=x.columns[52:],columns=[&quot;coef&quot;]) frames = [ranks_df, features_df] all_features_df = pd.concat(frames) all_features_df.sort_values(&quot;coef&quot;,ascending=False) . coef . r1_K | 0.907519 | . r2_K | 0.618435 | . triple_suit | 0.601521 | . paired | 0.494529 | . K | 0.342059 | . ... | ... | . r1_J | -0.361248 | . r1_T | -0.362056 | . r2_J | -0.407798 | . one_suit | -0.703227 | . non_paired | -1.150538 | . 72 rows × 1 columns . rr1 = np.zeros(shape=(1,13), dtype = np.float64) for idx, i in enumerate(coefs[0][62:75]): rr1[0][idx]+=i fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(13), ranks) plt.yticks([]) plt.imshow(rr1,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(), coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.title(&quot;Rank 1&quot;) plt.grid(False) rr2 = np.zeros(shape=(1,13), dtype = np.float64) for idx, i in enumerate(coefs[0][75:88]): rr2[0][idx]+=i fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(13), ranks) plt.yticks([]) plt.imshow(rr2,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(), coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.title(&quot;Rank 2&quot;) plt.grid(False) rr3 = np.zeros(shape=(1,13), dtype = np.float64) for idx, i in enumerate(coefs[0][88:101]): rr3[0][idx]+=i fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(13), ranks) plt.yticks([]) plt.imshow(rr3,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(), coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.title(&quot;Rank 3&quot;) plt.grid(False) . rr4 = np.zeros(shape=(1,10), dtype = np.float64) for idx, i in enumerate(coefs[0][101:]): rr4[0][idx]+=i fig, ax = plt.subplots(1,1, figsize=(20,10)) plt.xticks(range(0,len(x.columns[101:])), x.columns[101:]) plt.yticks([]) plt.imshow(rr4,interpolation=&#39;nearest&#39;) bins = np.linspace(coefs.min(),coefs.max(), 5) plt.colorbar(shrink=.3, ticks=bins) plt.title(&quot;Rank types&quot;) plt.grid(False) . from joblib import dump, load dump(model, DATA_DIR / &quot;model.joblib&quot;) model2 = load(DATA_DIR / &quot;model.joblib&quot;) . . Model 2 : adding features . . suited=[] for f in flops.index: s = set([i for i in f if i in suits]) suited.append(len(s)) . flops[&quot;one_suit&quot;]=[1 if i == 1 else 0 for i in suited] flops[&quot;double_suit&quot;]=[1 if i == 2 else 0 for i in suited] flops[&quot;triple_suit&quot;]=[1 if i == 3 else 0 for i in suited] . kind=[] for f in flops.index: r = set([i for i in f if i in ranks]) kind.append(len(r)) . flops[&quot;non_paired&quot;]=[1 if i == 3 else 0 for i in kind] flops[&quot;paired&quot;]=[1 if i == 2 else 0 for i in kind] flops[&quot;trip&quot;]=[1 if i == 1 else 0 for i in kind] . zero_gap=[] for i in range(0,11): zero_gap.append([i,i+1,i+2][::-1]) zero_gap.append([12,1,0]) zero_gap . [[2, 1, 0], [3, 2, 1], [4, 3, 2], [5, 4, 3], [6, 5, 4], [7, 6, 5], [8, 7, 6], [9, 8, 7], [10, 9, 8], [11, 10, 9], [12, 11, 10], [12, 1, 0]] . gap=[] for f in flops.index: r = [i for i in f if i in ranks] r_idx = [ranks.index(i) for i in r if i in ranks] if r_idx in zero_gap: gap.append(1) else: gap.append(0) . flops[&quot;zero_gap&quot;]=gap flops[flops[&quot;zero_gap&quot;]==1].head() . equity ev bet check above_ev one_suit double_suit triple_suit non_paired paired trip zero_gap . flops . 4c3c2c | 49.233 | 9.338 | 25.067 | 74.933 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | . 4c3c2d | 48.529 | 9.266 | 64.791 | 35.209 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4c3d2c | 48.562 | 9.297 | 64.934 | 35.066 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4d3c2c | 48.559 | 9.279 | 64.789 | 35.211 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4h3d2c | 48.191 | 9.290 | 62.303 | 37.697 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | . one_gap=[] for i in range(0,10): one_gap.append([i,i+1,i+3][::-1]) one_gap.append([i,i+2,i+3][::-1]) one_gap.append([12,2,0]) one_gap.append([12,2,1]) # for g in one_gap: # print(ranks[g[0]], ranks[g[1]], ranks[g[2]]) gap=[] for f in flops.index: r = [i for i in f if i in ranks] r_idx = [ranks.index(i) for i in r if i in ranks] if r_idx in one_gap: gap.append(1) else: gap.append(0) flops[&quot;one_gap&quot;]=gap two_gaps=[] for i in range(0,9): two_gaps.append([i,i+2,i+4][::-1]) two_gaps.append([i,i+3,i+4][::-1]) two_gaps.append([i,i+1,i+4][::-1]) two_gaps.append([12,3,0]) two_gaps.append([12,3,1]) # for g in two_gaps: # print(ranks[g[0]], ranks[g[1]], ranks[g[2]]) gap=[] for f in flops.index: r = [i for i in f if i in ranks] r_idx = [ranks.index(i) for i in r if i in ranks] if r_idx in two_gaps: gap.append(1) else: gap.append(0) flops[&quot;two_gaps&quot;]=gap gap=[] for f in flops.index: r = [i for i in f if i in ranks] r_idx = [ranks.index(i) for i in r if i in ranks] if r_idx not in two_gaps and r_idx not in one_gap and r_idx not in zero_gap and len(set(r))==3: gap.append(1) else: gap.append(0) flops[&quot;gap_others&quot;]=gap . # [i for i in combinations([&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;], r=3)] . features = np.zeros(shape = (len(flops),52), dtype=np.float32) for flop in range(0, len(flops)): idx1,idx2,idx3 = flops.index[flop][0:2],flops.index[flop][2:4],flops.index[flop][4:6] #crds=list() for idx, card in enumerate(all_cards): #print card if (idx1 == card) or (idx2 == card) or (idx3 == card): features[flop][idx]+=1 df_features = pd.DataFrame(features, dtype=np.float,index=flops.index) x = df_features.join(flops.drop([&#39;equity&#39;,&#39;ev&#39;,&#39;bet&#39;,&#39;check&#39;,&#39;above_ev&#39;,&#39;p_0&#39;,&#39;p_1&#39;,&#39;true_p&#39;],axis=1)) . x.shape . (1755, 62) . sorted_binary_features = x.iloc[:, 52:].mean().sort_values() ax = sorted_binary_features.plot(kind=&#39;barh&#39;, stacked=True, figsize=(5, 12), title=&#39;Prevalence of Binary Features&#39;) ax.set_xlabel(&#39;Proportion of flops&#39;); . Model 3 : rank 1, rank 2 and rank 3 . rr = np.zeros(shape = (len(flops),39), dtype=np.float32) for idx, f in enumerate(df_features.index): rks = [getRank(ix) for ix, i in enumerate(df_features.loc[f]) if i==1] rks = sort(rks)[::-1] rr[idx][rks[0]]+=1 rr[idx][rks[1]+13]+=1 rr[idx][rks[2]+26]+=1 cols=[] for i in range(1,4): for j in range(0,13): col = &quot;r&quot;+str(i)+&quot;_&quot;+ranks[j] cols.append(col) rr_features=pd.DataFrame(rr, dtype=np.float,index=flops.index,columns=cols) x = x.join(rr_features) . x.shape . (1755, 101) . sorted_binary_features = x.iloc[:, 62:75].mean().sort_values() ax = sorted_binary_features.plot(kind=&#39;barh&#39;, stacked=True, figsize=(5, 12), title=&#39;High Card Flop Prevalence&#39;) ax.set_xlabel(&#39;Proportion of flops&#39;) ax.set_yticks(range(0,13)) ax.set_yticklabels(ranks) plt.show() . sorted_binary_features = x.iloc[:, 75:88].mean().sort_values() ax = sorted_binary_features.plot(kind=&#39;barh&#39;, stacked=True, figsize=(5, 12), title=&#39;Second Card Flop Prevalence&#39;) ax.set_xlabel(&#39;Proportion of flops&#39;) ax.set_yticks(range(0,13)) ax.set_yticklabels(sorted_binary_features.index.str.replace(&quot;r2_&quot;,&quot;&quot;)) plt.show() . sorted_binary_features = x.iloc[:, 88:].mean().sort_values() ax = sorted_binary_features.plot(kind=&#39;barh&#39;, stacked=True, figsize=(5, 12), title=&#39;Third Card Flop Prevalence&#39;) ax.set_xlabel(&#39;Proportion of flops&#39;) ax.set_yticks(range(0,13)) ax.set_yticklabels(sorted_binary_features.index.str.replace(&quot;r3_&quot;,&quot;&quot;)) plt.show() . rank_types = [&#39;HHH&#39;,&#39;HHM&#39;,&#39;HHL&#39;,&#39;HMM&#39;,&#39;HML&#39;,&#39;HLL&#39;,&#39;MMM&#39;,&#39;MML&#39;,&#39;MLL&#39;,&#39;LLL&#39;] rr_type = np.zeros(shape = (len(flops),10), dtype=np.float32) for idx, f in enumerate(flops.index): r = [i for i in f if i in ranks] r_idx = [ranks.index(i) for i in r if i in ranks] _str=&quot;&quot; for rk in r_idx: if rk&gt;=8: _str+=&quot;H&quot; elif rk&lt;8 and rk&gt;3: _str+=&quot;M&quot; else: _str+=&quot;L&quot; for rt, i in enumerate(rank_types): if i == _str: rr_type[idx][rt]+=1 rr_type_features=pd.DataFrame(rr_type, dtype=np.float,index=flops.index,columns=rank_types) x = x.join(rr_type_features) . ValueError Traceback (most recent call last) &lt;ipython-input-176-b69471c010cf&gt; in &lt;module&gt; 17 18 rr_type_features=pd.DataFrame(rr_type, dtype=np.float,index=flops.index,columns=rank_types) &gt; 19 x = x.join(rr_type_features) ~ Anaconda3 lib site-packages pandas core frame.py in join(self, other, on, how, lsuffix, rsuffix, sort) 7244 # For SparseDataFrame&#39;s benefit 7245 return self._join_compat( -&gt; 7246 other, on=on, how=how, lsuffix=lsuffix, rsuffix=rsuffix, sort=sort 7247 ) 7248 ~ Anaconda3 lib site-packages pandas core frame.py in _join_compat(self, other, on, how, lsuffix, rsuffix, sort) 7267 right_index=True, 7268 suffixes=(lsuffix, rsuffix), -&gt; 7269 sort=sort, 7270 ) 7271 else: ~ Anaconda3 lib site-packages pandas core reshape merge.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate) 81 validate=validate, 82 ) &gt; 83 return op.get_result() 84 85 ~ Anaconda3 lib site-packages pandas core reshape merge.py in get_result(self) 646 647 llabels, rlabels = _items_overlap_with_suffix( --&gt; 648 ldata.items, lsuf, rdata.items, rsuf 649 ) 650 ~ Anaconda3 lib site-packages pandas core reshape merge.py in _items_overlap_with_suffix(left, lsuffix, right, rsuffix) 2009 raise ValueError( 2010 &#34;columns overlap but no suffix specified: &#34; -&gt; 2011 &#34;{rename}&#34;.format(rename=to_rename) 2012 ) 2013 ValueError: columns overlap but no suffix specified: Index([&#39;HHH&#39;, &#39;HHM&#39;, &#39;HHL&#39;, &#39;HMM&#39;, &#39;HML&#39;, &#39;HLL&#39;, &#39;MMM&#39;, &#39;MML&#39;, &#39;MLL&#39;, &#39;LLL&#39;], dtype=&#39;object&#39;) . x.shape . (1755, 111) . sorted_binary_features = x.iloc[:, 101:].mean().sort_values() ax = sorted_binary_features.plot(kind=&#39;barh&#39;, stacked=True, figsize=(5, 12), title=&#39;Rank Types Prevalence&#39;) ax.set_xlabel(&#39;Proportion of flops&#39;) ax.set_yticks(range(0,10)) ax.set_yticklabels(sorted_binary_features.index) plt.show() . Building the isomorphic 1755 flops . from itertools import combinations, permutations, product al_flops = [&#39;&#39;.join(combination) for combination in combinations(all_cards, r=3)] . len(al_flops) . all_flops = list() for f in al_flops: r = [i for i in f if i in ranks] s = [i for i in f if i in suits] r_idx = np.array([ranks.index(i) for i in r if i in ranks]) idx= np.argsort(r_idx)[::-1] flop = r[idx[0]] + s[idx[0]] + r[idx[1]] + s[idx[1]] + r[idx[2]] + s[idx[2]] all_flops.append(flop) . final_set = set() . c = 0 #final_set = set() to_remove=list() for idx, f in enumerate(all_flops): s = set([i for i in f if i in suits]) if len(s)==1: c+=1 iso = f.replace(list(s)[0],&quot;h&quot;) final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set)) #good . 1144/286 . c = 0 to_remove=list() for idx, f in enumerate(all_flops): r = [i for i in f if i in ranks] s = [i for i in f if i in suits] if len(set(r))==3 and len(set(s))==2: c+=1 if s[0] == s[1]: iso = r[0] + &quot;h&quot; + r[1] + &quot;h&quot; + r[2] + &quot;d&quot; if s[0] == s[2]: iso = r[0] + &quot;h&quot; + r[1] + &quot;d&quot; + r[2] + &quot;h&quot; if s[1] == s[2]: iso = r[0] + &quot;h&quot; + r[1] + &quot;d&quot; + r[2] + &quot;d&quot; final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set)) #good . c = 0 to_remove=list() for idx, f in enumerate(all_flops): r = [i for i in f if i in ranks] s = [i for i in f if i in suits] if len(set(s))==3 and len(set(r))==3: c+=1 iso = r[0] + &quot;h&quot; + r[1] +&quot;d&quot; + r[2] + &quot;c&quot; final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set)) . c = 0 to_remove=list() for idx, f in enumerate(all_flops): r = [i for i in f if i in ranks] #s = set([i for i in f if i in suits]) if len(set(r))==1: c+=1 iso = r[0] + &quot;h&quot; + r[0] +&quot;d&quot; + r[0] + &quot;c&quot; final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set))#good . c = 0 to_remove=list() for idx, f in enumerate(all_flops): r = [i for i in f if i in ranks] s = [i for i in f if i in suits] if len(set(r))==2 and len(set(s))==3: c+=1 iso = r[0] + &quot;h&quot; + r[1] + &quot;d&quot; + r[2] + &quot;c&quot; final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set)) #good . c = 0 to_remove=list() for idx, f in enumerate(all_flops): r = [i for i in f if i in ranks] s = [i for i in f if i in suits] if len(set(r))==2 and len(set(s))==2: c+=1 if r[0]== r[1]: iso = r[0] + &quot;h&quot; + r[1] + &quot;d&quot; + r[2] + &quot;d&quot; if r[1]== r[2]: iso = r[0] + &quot;h&quot; + r[1] + &quot;h&quot; + r[2] + &quot;d&quot; #if s[1]== s[2]: #iso = r[0] + &quot;h&quot; + r[1] + &quot;d&quot; + r[2] + &quot;d&quot; final_set.add(iso) to_remove.append(idx) all_flops = [i for idx, i in enumerate(all_flops) if idx not in to_remove] print(c) print(len(all_flops)) print(len(final_set)) #wrong . 1144/22100 == 286/1755 . c=0 for f in list(final_set): r = [i for i in f if i in ranks] s = [i for i in f if i in suits] if r[0]==&quot;A&quot; and r[1]==&quot;K&quot; and len(set(s))==2: print(f) c+=1 print(c) . [&#39;&#39;.join(combination) for combination in combinations(suits, r=3)] . [&#39;&#39;.join(combination) for combination in combinations(suits, r=2)] . [&#39;&#39;.join(combination) for combination in product(suits, repeat=2)] . r = [1,2,3] if len(set(r))==3: r1, r2, r3 = r[0], r[1], r[2] print(r1) print(r2) print(r3) .",
            "url": "https://h7r.github.io/fast/2022/01/16/pk_processing_flops.html",
            "relUrl": "/2022/01/16/pk_processing_flops.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Seasonality in Boston Index Crime rates",
            "content": "#prettify tables from IPython.display import display #prevent from warnings from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates #%pylab inline plt.style.use(&#39;fivethirtyeight&#39;) . . &quot;Truth is not always in a well. In fact, as regards the more important knowledge, I do believe that she is invariably superficial. The depth lies in the valleys where we seek her, and not upon the mountain-tops where she is found.&quot; . &quot;It will be found, in fact, that the ingenious are always fanciful, and the truly imaginative never otherwise than analytic.&quot; . — Edgar Allan Poe, The Murders in the Rue Morgue (1841) . Be like a sniffer dog . This is a story of peaks and valleys, like in a Dow Jones chart. &quot;Pictures are undervalued in science&quot;, wrote Benoît Mandelbrot in his excellent book The (Mis)Behavior of Markets. We agree with the French mathematician. We believe that pictures, good pictures and not ridiculograms [1], are the royal road to epiphanies. . In this study, charts helped us a lot. Despite the noisy data, an unexpected pattern was showing up insistently from the very start: like temperature or influenza, crime is seasonal. Peak fever usually occurs in summer. Evidences pop up all over the data. Maybe it will be found, to paraphrase Edgar Allan Poe, that data analysts are never otherwise than sniffer dogs following a promising track. . Once the target has been identified, we have narrowed the dataset originally collected by the Boston Police Department (BPD) [2] to focus only on the offenses belonging to the said &quot;Crime Index&quot;: . [...] they are the crimes most likely to be reported and most likely to occur with sufficient frequency to provide an adequate basis for comparison. [...] The offenses included in the Crime Index total are the violent crimes of Murder and Nonnegligent Manslaughter, Forcible Rape, Robbery, and Aggravated Assault, and the property crimes of Burglary, Larceny-theft, and Motor Vehicle Theft. Arson is excluded due to inconsistent reporting. [3] . Then, using Singular Spectrum Analysis (SSA) technique like Dong et al. (2017) [4], our main reference for this study, long term trends and seasonal components for each crime category have been extracted for analysis purposes. We found that Index Crimes rates decrease following the general tendency observed in the US since decades, except for violent crimes, which are pretty stable, maybe because they reached a &quot;floor&quot;. Also, the data shows the importance of outliers in understanding how crime rates strongly react to dramatic events. . First clues of seasonality and lockdown effect . First, the BPD Crime Incident Reports (August 2015 to October 2020) shows seasonal variations though obscured by noisy data (Fig. 1). We need to build a tool to extract the long term trend. This is exactly what SSA is for: . The basic aim of SSA is to decompose the time series into the sum of interpretable components such as trend, periodic components and noise with no a-priori assumptions about the parametric form of these components. [5] . However, if we look carefully at Fig. 1, a substantial hole is drawed at the end of the series. The phenomenon starts in fact shortly before the State of Emergency is declared on March 23, 2020 by Governor Baker, and ends after Reopening announcement [6] on May 18:this is what we call the lockdown effect (Fig. 2), i.e. an abnormal drop in the number of incidents due to the major part of the population being under strict control, empty streets and shops closed. After the economy reopen, the crime quickly reopened too, and climaxed on June 1 for then coming back to its old routine made of sudden peaks and drops – the seasonal oscillations. . We will show infra that the June 1 spike is most likely related to George Floyd protests maybe not so peaceful but running hot in response to police brutality. . From this point of view, crime – crimes against property more precisely – is nothing but a kind of thermometer. Take it (for analysis) or break it (with imprisonment and incapacitation effect), the thermometer says that burglary, auto-theft and vandalism have been the best riots indicators during George Floyd protests (Fig. 6). Rinse, repeat: the data shows that this people didn&#39;t target persons, but property, at least at Boston. This is why we need good data science. Because truth matters. Don&#39;t be afraid to tell the truth. She is often an outlier, a surprising and beautiful outlier. In other words, truth is a &quot;dramatic change&quot; in the data. . [...] social catastrophes can override any seasonality and lead to dramatic changes in future crime rates. [4] . So, please, be careful when you remove outliers! . usecols=[0,3,7,10,12,14,15] index_col = &quot;OCCURRED_ON_DATE&quot; path=&quot;data/boston_2020-10-03.csv&quot; data=pd.read_csv(path, index_col=index_col, usecols=usecols, low_memory=False).sort_index() #rename columns col_names=[&quot;id&quot;,&quot;description&quot;,&quot;day_of_the_week&quot;,&quot;ucr&quot;,&quot;lat&quot;,&quot;long&quot;] to_rename=dict() for idx, col in enumerate(data.columns): to_rename[col]=col_names[idx] data=data.rename(columns=to_rename) #rename index data.index.rename(&quot;date&quot;,inplace=True) #set datetime type for index data.index = pd.to_datetime(pd.to_datetime(data.index).date) #print(data.index.dtype) #daily aggregate daily = data.index.value_counts(sort=False).sort_index() #print(daily.index.dtype) #remove last day for which report is incomplete daily=daily[0:len(daily)-1] #define some useful lists daily_index=daily.index months = [&#39;January&#39;, &#39;February&#39;, &#39;March&#39;, &#39;April&#39;, &#39;May&#39;, &#39;June&#39;, &#39;July&#39;, &#39;August&#39;, &#39;September&#39;, &#39;October&#39;, &#39;November&#39;, &#39;December&#39;] weekdays = [&quot;Monday&quot;,&quot;Tuesday&quot;,&quot;Wednesday&quot;,&quot;Thursday&quot;,&quot;Friday&quot;,&quot;Saturday&quot;,&quot;Sunday&quot;] . . #uncomment if you want to print this infos # print(&quot;Start at: &quot; + str(data.index[0])) # print(&quot;End at: &quot; + str(data.index[-1])) # print(&quot;Number of days: &quot; + str(len(data.index.value_counts()))) # print(&quot;Number of incidents (non uniques): &quot; + str(len(data))) . . #check if missing values. There are missing value for the columns &quot;ucr&quot; &quot;lat&quot; and &quot;long&quot; #but it doesn&#39;t matter for us #uncomment to print results #print(data.isnull().sum()) . . #check and remove duplicated values #uncomment to print #print(data.duplicated().sum()) data = data.drop_duplicates() #feel free to uncomment (sanity check) #print(data.duplicated().sum()) . . #print(data[&quot;description&quot;].value_counts(ascending=False)[:30]) . Fig. 1 shows also that offense types distribution is heavily unbalanced, exhibing a large fat tail typical of a Pareto distribution right skewed. As a matter of fact, 10.5% of offense types account for 72.4% of the incidents, which is very close to the 80/20 rule [7]. . Therefore, if we want to evaluate correctly the success —or the failure— of the public safety policy at Boston, it would be worth to focus only on the most relevant offense categories. . . locator = mdates.MonthLocator(12) fmt = mdates.DateFormatter(&#39;%b. %Y&#39;) fig, ax = plt.subplots(2,2, figsize=(20, 15)) #fig.suptitle(&quot;Fig. 1 - Crime Incident Reports overview&quot;, fontsize=&quot;30&quot;) ax[0,0].hist(daily) ax[0,0].set_title(&quot;Daily incidents&quot;) ax[0,0].set_xlabel(&quot;# of incidents&quot;) ax[0,0].set_ylabel(&quot;# of days&quot;) ax[1,0].xaxis.set_major_locator(locator) ax[1,0].xaxis.set_major_formatter(fmt) ax[1,0].set_xlim(daily.index[0],daily.index[-1]) ax[1,0].plot(daily.index,daily) ax[1,0].set_ylabel(&quot;# of incidents&quot;) ax[0,1].hist(data[&quot;description&quot;].value_counts()) ax[0,1].set_xlabel(&quot;# of incidents&quot;) ax[0,1].set_ylabel(&quot;# of offense types&quot;) ax[0,1].set_title(&quot;Offense types&quot;) sum_counts = np.sum(data[&quot;description&quot;].value_counts()) cdf = data[&quot;description&quot;].value_counts(ascending=False).cumsum()[:30] ax[1,1].bar(range(30),cdf/sum_counts) ax[1,1].set_xlabel(&quot;# of offense types&quot;) ax[1,1].set_xticks(range(0,45,15)) ax[1,1].set_ylabel(&quot;cumulative distribution&quot;) #ax[1,1].set_title(&quot;Top-30&quot;) plt.show() . . Fig. 1. — Crime Incident Reports overview. While the total number of incidents in Boston rarely exceeds 300 events on a single day, following what looks like a perfect Gaussian distribution, the raw daily data is obscured by seasonal oscillations and a certain amount of stochasticity. Note the sudden drop in the third chart: it occurs right after State of Emergency is declared on March 23, 2020. When the Reopening is announced on May 18, the number of daily incidents increases first slowly then quickly a few days after, hitting a local maximum on June 1. . . #plot Covid lockdown window fig, ax = plt.subplots(figsize=(20, 10)) mn, mx = daily[&#39;2018-03-23&#39;:].min(), daily[&#39;2018-03-23&#39;:].max() ax.plot(daily[&#39;2018-03-23&#39;:]) ax.set_ylabel(&quot;# of incidents&quot;) #ax.set_xlim(&#39;2019-03-23&#39;,daily.index[-1]) ax.fill_between((&#39;2020-03-23&#39;,&#39;2020-05-18&#39;), mn, mx, facecolor=&#39;orange&#39;, alpha=0.2) plt.show() . . Fig. 2. — Zoom on lockdown effect. A significative drop in crime rate occurs during the lockdown at Boston (orange shape). The short term oscillations (seasonality) are not immune to brutal social changes. We&#39;ll show *infra* that the spike occurring on June 1 is due to George Floyd protests. . Before starting, let&#39;s make another picture of the data. . A heatmap is a useful visualization technique to show in two dimensions the magnitude of a phenomenon with color variations. Since the Fig. 1 gives some clues of seasonality we decided to investigate, by reorganizing our daily time series into a 2d-array —x-axis for weekdays, y-axis for months—, the seasonal pattern if present should become evident once generated in the appropriate visual format. . . . plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(1,1, figsize=(15,5)) plt.yticks(range(7), weekdays) plt.xticks(range(12), months, rotation=&#39;vertical&#39;) ar = np.zeros(shape=(7,12),dtype=np.float32) for i in range(len(data)): d=data.index[i].weekday() m=data.index[i].month ar[d][m-1]+=1 img = ax.imshow(ar,plt.cm.Greens, interpolation=&#39;nearest&#39;) ax.grid(False) #ax.set_title(&quot;Fig. 3 - Daily incidents heatmap (by count)&quot;,fontsize=20) bins = np.linspace(ar.min(), ar.max(), 5) plt.colorbar(img, shrink=.6, ticks=bins) plt.show() . . Fig. 3. — Daily incidents heatmap (by count). The data have been reorganized in an array where each cell is the recipient of the total count of incidents occurred a given weekday of a given month of the year. The darkest zone around the middle of the figure indicates that the number of incidents is usually highest between June and September. . The color interpolation in Fig. 3 indicates a three/four-months range where incidents are reported at a highest frequency. For sure, it&#39;s a good rule of thumb to never draw conclusions too quickly from charts, but it&#39;s also true that they are very good food for thought. They help us a lot to navigate through the noisy data we try to understand better. So we would say this is a good start for us and hope future unexpected discoveries. . Next step, get a cleanest and narrowed data through feature selection. . Size doesn&#39;t matter: all we need is love and good data . By reducing our sample size and selecting carefully the offense types, our goal is to remove bias in the data collected by agents potentially involved in race for results (&quot;get those numbers and come back with them&quot;), which lead in the past to serious misconducts [13]. . The UCR Part I crimes [8], or Index Crimes, are divided in two categories: violent crimes (crime against persons) and property crimes (crime against property): . Violent Crimes Property Crimes . ASSAULT - AGGRAVATED | LARCENY | . MURDER | AUTO THEFT | . ROBBERY | ARSON | . RAPE | BURGLARY | . As said in introduction, arson offenses have been excluded from the Index Crimes due to inconsistent reporting. Due to inconsistency in the BPD dataset regarding sexual assault (change in law), we will also exclude the rapes from this study. . def offenseExtractor(df, _name): &quot;&quot;&quot; Inputs: df: a dataframe _name: a string or a regular expression Output: a filtered dataframe which description column contains _name input &quot;&quot;&quot; return(df.loc[(df[&#39;description&#39;].str.contains(_name))]) def getIndexCrimes(data, _names): &quot;&quot;&quot; Input: data (DataFrame): the data we want to create features (offense types) from. _names: a list of strings or regular expressions. Output: DataFrame: A DataFrame with index crimes as columns. Note: use pre-declared list daily_index &quot;&quot;&quot; offenses = pd.DataFrame(index=daily_index) for _name in _names: offenses[_name] = aggregateBuilder(offenseExtractor(data, _name)) return offenses def aggregateBuilder(df): &quot;&quot;&quot; Inputs: df: a time series DataFrame Output: a daily aggregate (Series) Note: use pre-declared list daily_index &quot;&quot;&quot; #aggregate by day of the year and sort the index s = df.index.value_counts(sort=False).sort_index() #reindex with daily_index and fill the missing days with 0 as value s = s.reindex(daily_index, fill_value=0) return s def prepareHeatmap(df): #rename function? &quot;&quot;&quot; input: a time series output: a numpy array with shape(7,12) which x-axis = weekday and y-axis = month &quot;&quot;&quot; ar = np.zeros(shape=(7,12),dtype=np.float32) for i in range(len(df)): d=df.index[i].weekday() m=df.index[i].month ar[d][m-1]+=df.iloc[i] return ar . . keywords=[&quot;ASSAULT - AGGRAVATED&quot;, &quot;ROBBERY&quot;, &quot;MURDER&quot;, &quot;LARCENY&quot;, &quot;BURGLARY&quot;, &quot;AUTO THEFT&quot;] index_crimes = getIndexCrimes(data, keywords) . . index_crimes[&quot;violent_crimes&quot;]=index_crimes.iloc[:,0:3].sum(axis=1) index_crimes[&quot;property_crimes&quot;]=index_crimes.iloc[:,3:6].sum(axis=1) index_crimes[&quot;part_one_crimes&quot;]=index_crimes.iloc[:,6:8].sum(axis=1) #uncomment if you want to display the dataframe #display(index_crimes.head()) . . The Part I crimes as well as the property crimes heatmaps (Fig. 4) is like a reduced snapshot of the Fig. 3. . The same peak in (late) summer is still showing up despite we have strongly narrowed the data. It looks like the phenomena of seasonality is strong enough to be sample reduction resistant. . . fig = plt.figure(figsize = (15,10)) titles= [&quot;Part I Crimes&quot;, &quot;Violent Crimes&quot;, &quot;Property Crimes&quot;] #plt.yticks(range(21), weekdays*3) #plt.xticks(range(12), months, rotation=90) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) #plt.grid(False) #plt.set_xticklabels(range(12), months, rotation=&quot;vertical&quot;) #fig.suptitle(&quot;Fig. 3 - Most serious crimes&quot;, fontsize=30) #plt.label_outer() #plt.set_xticks(range(12), weekdays) for i in range(0,3): #img = ax[idx//2][0] if i == 0: df = prepareHeatmap(index_crimes.part_one_crimes) ax1.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax1.set_title(titles[i]) ax1.set_yticks(range(7)) ax1.set_yticklabels(weekdays) ax1.set_xticks(range(12)) ax1.set_xticklabels(months,rotation=90) ax1.grid(False) elif i== 1: df = prepareHeatmap(index_crimes.violent_crimes) ax2.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax2.set_title(titles[i]) ax2.set_yticks([]) #ax2.set_yticklabels(weekdays) ax2.set_xticks(range(12)) ax2.set_xticklabels(months,rotation=90) ax2.grid(False) else: df = prepareHeatmap(index_crimes.property_crimes) ax3.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax3.set_title(titles[i]) ax3.set_yticks([]) #ax3.set_yticklabels(weekdays) ax3.set_xticks(range(12)) ax3.set_xticklabels(months,rotation=90) ax3.grid(False) #title=title[idx] + &quot; (&quot; + str(int(np.sum(df))) + &quot; obs.)&quot; #plt.title(title) #ax.set_xticklabels(months, rotation=&quot;vertical&quot;) #fig.delaxes(ax[4,1]) . . Fig. 4. — The most serious crimes by count. At first glance, Black Friday sales for crime shopping occurs in July. Spot seven differences between the left and the right heatmap is certainly a tough game. Sorry, no prize for the winner! It means that the weight of property crimes far exceeds the weight of violent crimes. . However, the violent crimes rate is not converging so well toward a clear seasonal peak, as if spotting the crimes against persons was in fact the real challenge here. . Is Boston doing better or worse than expected compared to other American cities? . By performing SSA, we will be able to answer the question in the next section. . The trajectory space of the crime . We start by quoting again Dong et al. (2017), our main source of inspiration for this work: . The first step in the SSA involves creating a trajectory matrix. For a time series $X = (x_1$, $x_2$,...,$x_N)$ and a window length $L = 365$, we define the trajectory matrix as: begin{equation*}X = begin{pmatrix} x_1 &amp; x_2 &amp; x_3 &amp; cdots &amp; x_K x_2 &amp; x_3 &amp; x_4 &amp; cdots &amp; x_{K+1} x_3 &amp; x_4 &amp; x_5 &amp; cdots &amp; x_{K+2} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots x_L &amp; x_{L+1} &amp; x_{L+2} &amp; vdots &amp; x_N end{pmatrix} end{equation*} . with $K = N - L + 1$. Then a singular value decomposition (SVD) is performed on the trajectory matrix, providing a decomposition $X = X_1 + ⋯ + X_L$. Each of our modes then comes from averaging these matrices $X_i$ along their anti-diagonal. The mode with the largest singular value always corresponds to the average crime trend. We then typically would look for any modes with an annual periodicity in the first twenty singular values and sum these to produce our seasonal component. . Since we work under two constraints, time $t ne infty$ and budget $b approx0$, we don&#39;t go into detail. For those interested in learning more about SSA, see [9] and [10]. . Our purpose is to applying SSA technique to Boston following [4] guidelines. . from scipy.linalg import svd, diagsvd . . #creating a trajectory matrix def createLaggedVectors(s, L): &quot;&quot;&quot; inputs: s : series or 1d array-like L : an integer (the window length - long-term timeframe we want to extract) output: a 2d numpy array (Hankel matrix - all the elements along the diagonal are equal) &quot;&quot;&quot; N = len(s) K = N - L + 1 X = np.zeros(shape=(L,K),dtype=np.float64) for i in range (0,L): for j in range(0,K): X[i][j]+= s[j+i] return X def componentsExtractor(s,L): &quot;&quot;&quot; inputs: s : series or 1d array-like L : an integer (the window length - long-term timeframe we want to extract) output: a 2d numpy array of shape (len(s) x L) &quot;&quot;&quot; N = len(s) #trajectory matrix creation X = createLaggedVectors(s,L) #SVD decomposition avec scipy.linalg.svd U, s, VT = svd(X) #rank of the trajectory matrix, most of time d=L (no lineary dependence between columns of X) d = np.linalg.matrix_rank(X) #reconstruction X_components = np.zeros((N, L)) #thanks to Joan d&#39;Arcy Kaggle notebook [10] for the code below for i in range(0,d): #the tricky part: eigentriple grouping X_elem = s[i]*np.outer(U[:,i], VT[i,:]) X_rev = X_elem[::-1] #diagonal averaging X_components[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])] return X_components def checkAccuracy(s,ssa_array): &quot;&quot;&quot; Check if no error rebuilding the data time series from the eigentriples array inputs: s : series or 1d array-like ssa_array : 2d numpy array containing the eigentriples as columns output: True or False &quot;&quot;&quot; return np.allclose(s,np.sum(ssa_df,axis=1)) . . #uncomment for sanity check example #violent_crimes_ssa = componentsExtractor(index_crimes.violent_crimes, 365) # print(checkAccuracy(index_crimes.violent_crimes,violent_crimes_ssa)) . . #daily_ssa = componentsExtractor(daily, 365) part_one_crimes_ssa = componentsExtractor(index_crimes.part_one_crimes, 365) violent_crimes_ssa = componentsExtractor(index_crimes.violent_crimes, 365) property_crimes_ssa = componentsExtractor(index_crimes.property_crimes, 365) #separate trend component from seasonal component part_one_crimes_trend = pd.Series(part_one_crimes_ssa[:,0],daily_index) part_one_crimes_seasonal = pd.Series(part_one_crimes_ssa[:,0:3].sum(axis=1),daily_index) violent_crimes_trend = pd.Series(violent_crimes_ssa[:,0],daily_index) violent_crimes_seasonal = pd.Series(violent_crimes_ssa[:,0:3].sum(axis=1),daily_index) property_crimes_trend = pd.Series(property_crimes_ssa[:,0],daily_index) property_crimes_seasonal = pd.Series(property_crimes_ssa[:,0:3].sum(axis=1),daily_index) . . . titles= [&quot;Part I Crimes&quot;, &quot;Violent Crimes&quot;, &quot;Property Crimes&quot;] fig = plt.figure(figsize = (15,15)) ax1 = fig.add_subplot(311) ax2 = fig.add_subplot(312) ax3 = fig.add_subplot(313) #fig.suptitle(&quot;Fig. 4 - Most serious crimes with SSA components&quot;,fontsize=30) for i in range(0,3): if i == 0: ax1.plot(index_crimes.part_one_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax1.plot(part_one_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax1.plot(part_one_crimes_seasonal,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax1.set_title(titles[i]) ax1.set_xticks([]) ax1.legend() elif i== 1: ax2.plot(index_crimes.violent_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax2.plot(violent_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax2.plot(violent_crimes_seasonal ,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax2.set_title(titles[i]) ax2.set_xticks([]) ax2.set_ylabel(&quot;# of crimes&quot;, fontsize=20) else: ax3.plot(index_crimes.property_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax3.plot(property_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax3.plot(property_crimes_seasonal ,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax3.set_title(titles[i]) plt.show() . . Fig. 5. — The most serious crimes with their SSA components. Daily aggregates are plotted in grey, long term trends in yellow, and the green curves are the long term trends plus the first two seasonal components (ET1-3). See the big outlier on the right? . Recap: . SSA technique makes a decomposition of a time series $X$ of length $N$ (the diagonal of the trajectory matrix) into a sum of $L$ components of length $N$. To reconstruct the time series, simply sum the $L$ components and make a sanity check. | The mode with the largest values, i.e. the first elementary reconstructed series or first eigentriple (ET1), is the long term trend. | We sum the first other modes with annual periodicity to produce the seasonal component. | . Long term trends extraction (the signal) . The Index Crimes in Boston are slowly falling accordingly to the national tendency [11], but this is due above all to a sharp decline in property crimes frequency, while violent crimes long term trend is flat as a hell (Fig. 5). Due to a very low amplitude, annual variations for violent crimes would be almost impossible to detect without the SSA technique. . Back to the original dataset. Three offense types exhibe the same outlier (Fig. 6): . BURGLARY - COMMERICAL (sic) | AUTO THEFT | VANDALISM | . Quick research confirm that George Floyd protests in Boston streets on May 31 have ended in violence [12]. The many incidents have been reported by BDP one day later, on June 1, resulting in the outlier in the data. . We will now emphasize both violent crimes and property crimes long trends by zooming on their respective subcategories. . . plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;BURGLARY - COMMERICAL&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;BURGLARY - COMMERCIAL&quot;) plt.show() plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;AUTO THEFT&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;AUTO THEFT&quot;) plt.show() plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;VANDALISM&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;VANDALISM&quot;) plt.show() . . Fig. 6. — Boston protests against George Floyd killing ending in violence. Commercial burglary, auto theft and vandalism exhibe the same spike on June 1, showing that these three offense types are the best markers for riots and looting. . Though we observe a general decrease for Part I Crimes (Fig. 5), the aggravated assaults are slightly on the rise and auto thefts have ceased to decrease after 2019, while the rate of decrease for burglary seems to be slowing (Fig. 7). As mentioned by [4]: . On possible cause is that crime is at very low basal level, and it is hard to decrease it below this level. . violent_crimes_list = [&#39;ASSAULT - AGGRAVATED&#39;, &#39;MURDER&#39;, &#39;ROBBERY&#39;] property_crimes_list = [&#39;LARCENY&#39;, &#39;AUTO THEFT&#39;, &#39;BURGLARY&#39;] years=[2015,2016,2017,2018,2019,2020,2021] . . . fig, ax = plt.subplots(2,1, figsize=(10,10)) ylabel = &quot;Trend $C$/$C_0$&quot; for i in range(0,2): #plt.suptitle(&quot;Fig. 5 - Long term trends by subcategories&quot;, fontsize=30) if i == 0: off_list = violent_crimes_list ax[0].set_xticks([]) title=&quot;Violent crimes trend&quot; else: off_list = property_crimes_list ax[1].set_xticklabels(years) title=&quot;Property crimes trend&quot; for idx, offense in enumerate(off_list): ssa = componentsExtractor(index_crimes[offense], 365)[:,0] trend = pd.Series(ssa, index=daily_index) #normalize the values s = (1+trend.pct_change()).cumprod().fillna(1) ax[i].plot(s) ax[i].set_ylabel(ylabel, fontsize=20) ax[i].set_ylim(0.4,1.4) ax[i].set_yticklabels([&quot;&quot;,0.6,0.8,1.,1.2,&quot;&quot;]) ax[i].set_title(title) ax[i].legend(off_list, loc=&quot;lower left&quot;) plt.show() . . Fig. 7. — Long term trends by subcategories. Following Dong et al. (2017), the data has been normalized &quot;to be the fraction of the initial crime level for the first time point $C_0$&quot;. Murder trend is pretty erratic, but keep in mind that homicides occur very rarely (less than 1 by day in average), so the variance is higher. . This is a paradox: once reached very low levels of crime (proof of agents good work), it becomes harder to lower more the curve, and more likely to get bad results in the future (not a proof of agents bad work), losing therefore public funds. From this point of view, a public safety policy based only on quantified results is suboptimal and unfair to the agents, if not stressful for them (incentivizing the dangerous &quot;race for results&quot; pointed supra). . Seasonal components extraction (the noise) . To reconstruct the seasonal data, we will use the first $n$ leading SSA order modes, where $2 leq n leq4$, depending on the offense type, each one having different periodicity. . Again, we have followed the procedure described in [4]: . The trend is subtracted from the data and then the data is smoothed using a moving window average. As the oscillations are related to the total amount of crime, the data and seasonal component have been divided by the trend pointwise in time to normalize the oscillations. . Since our long term window length $L = 365$, we have smoothed the data with a monthly window average of length $l = 30$ before normalizing the oscillations. . . sorted_keywords = [&#39;ASSAULT - AGGRAVATED&#39;, &#39;LARCENY&#39;, &#39;BURGLARY&#39;,&#39;ROBBERY&#39;, &#39;AUTO THEFT&#39;] fig = plt.figure(figsize = (15,20)) #plt.yticks(range(21), weekdays*3) #plt.xticks(range(12), months, rotation=90) ax1 = fig.add_subplot(511) ax2 = fig.add_subplot(512) ax3 = fig.add_subplot(513) ax4 = fig.add_subplot(514) ax5 = fig.add_subplot(515) #plt.xticks(rotation=45) locator = mdates.MonthLocator(12) fmt = mdates.DateFormatter(&#39;%b. %Y&#39;) for i in range(0,5): daily_offenses = index_crimes[sorted_keywords[i]] title = sorted_keywords[i][0]+sorted_keywords[i][1:].lower() if i&lt;3: n=2 elif i==3: n=3 else: n=4 ssa = componentsExtractor(daily_offenses, 365)[:,:n+1] seasonal, trend = pd.Series(ssa[:,1:n+1].sum(axis=1), index=daily_index), pd.Series(ssa[:,0], index=daily_index) #put the seasonal component and the raw data on the same order smoothed_daily_offenses = (daily_offenses-trend).rolling(30,min_periods=1,center=True).mean() / trend seasonal = seasonal/trend if i== 0: ax1.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax1.plot(seasonal, label=&quot;Seasonal&quot;,color=&#39;orange&#39;) ax1.set_title(title) ax1.set_xticks([]) elif i== 1: ax2.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax2.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax2.set_title(title) ax2.set_xticks([]) elif i == 2: ax3.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax3.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax3.set_title(title) ax3.set_xticks([]) ax3.set_ylabel(&quot;Seasonal/Trend&quot;, fontsize=30) elif i == 3: ax4.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax4.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax4.set_title(title) ax4.set_xticks([]) else: ax5.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax5.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax5.set_title(title) ax5.xaxis.set_major_locator(locator) ax5.xaxis.set_major_formatter(fmt) ax5.set_xlim(smoothed_daily_offenses.index[0],smoothed_daily_offenses.index[-1]) . . Fig. 8. — Seasonal crime component. Even if the raw data has been smoothed, we see that the seasonal oscillations are fairly consistent, especially for aggravated assault, larceny and auto theft. In other words, each crime index type has a seasonal component. A large aberration appears for burglary during George Floyd protests, while a significant drop happens for robbery category during the lockdown, showing that robbery is the crime type that has been the most affected by State of Emergency.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . plt.figure(figsize=(10,5)) for i in range(0,5): daily_offenses = aggregateBuilder(offenseExtractor(data,sorted_keywords[i])) title = sorted_keywords[i][0]+sorted_keywords[i][1:].lower() if i&lt;3: n=2 elif i==3: n=3 else: n=4 ssa = componentsExtractor(daily_offenses, 365)[:,:n+1] seasonal, trend = pd.Series(ssa[:,1:n+1].sum(axis=1), index=daily_index), pd.Series(ssa[:,0], index=daily_index) plt.plot(seasonal, label=title) plt.legend(ncol=len(sorted_keywords), bbox_to_anchor=(0, 1), loc=&#39;lower left&#39;, fontsize=&#39;small&#39;) plt.show() . . Fig. 9. — Differences in seasonality.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; References . [1] Introduction to ridiculogram by M. E. J. Newman. Web. 2 July 2007. https://www.youtube.com/watch?v=YS-asmU3p_4 | [2] Analyse Boston - Crime Incident Reports (August 2015 - To Date) (Source: New System). (https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-new-system) | [3] USLegal - Index Crimes Law and Legal Definition. https://definitions.uslegal.com/i/index-crimes/ | [4] Dong, K., Cao, Y., Siercke, B., Wilber, M., &amp; McCalla, S. G. (2017). Advising caution in studying seasonal oscillations in crime rates. PloS one, 12(9), e0185432. https://doi.org/10.1371/journal.pone.0185432 | [5] Wikipedia - Singular spectrum analysis. https://en.wikipedia.org/wiki/Singular_spectrum_analysis | [6] Mass.gov - Reopening Massachusetts. May 18, 2020. [[PDF]](https://www.mass.gov/doc/reopening-massachusetts-may-18-2020/download) | [7] Wikipedia - Pareto principle. https://en.wikipedia.org/wiki/Pareto_principle | [8] FBI:UCR • Crimes in the U.S. 2010 • Offenses Definitions. https://ucr.fbi.gov/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/offense-definitions | [9] Golyandina, Nina &amp; Zhigljavsky, Anatoly. (2013). Singular Spectrum Analysis for Time Series. 10.1007/978-3-642-34913-3. https://www.researchgate.net/publication/260124592_Singular_Spectrum_Analysis_for_Time_Series | [10] Jordan D&#39;Arcy - Kaggle Notebook, Introducing SSA for Time Series Decomposition. https://www.kaggle.com/jdarcy/introducing-ssa-for-time-series-decomposition | [11] Brennan Center for Justice. Press Release. Crime Remains at Historic Lows in America. Web. June 12, 2018. https://www.brennancenter.org/our-work/analysis-opinion/crime-remains-historic-lows-america | [12] Boston Globe. By Jeremy C. Fox and John Hilliard. Boston protests against George Floyd killing begin peacefully, end in violence, arrests. Web. June 1, 2020. https://www.bostonglobe.com/2020/05/31/metro/three-protests-against-george-floyd-killing-planned-boston-sunday/ | [13] The New York Times. By Michelle Alexander. Why Police Lie Under Oath. Web. Feb. 2, 2013. https://www.nytimes.com/2013/02/03/opinion/sunday/why-police-officers-lie-under-oath.html?pagewanted=all&amp;_r=0 | . &lt;/div&gt; .",
            "url": "https://h7r.github.io/fast/notebook/jupyter/python/time%20series%20analysis/2022/01/16/boston-crime-report.html",
            "relUrl": "/notebook/jupyter/python/time%20series%20analysis/2022/01/16/boston-crime-report.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "The (un)expected variance at poker",
            "content": "Table of Contents . . import time import numpy as np import pandas as pd import matplotlib.pyplot as plt from pathlib import Path from functools import lru_cache from scipy import stats %pylab inline plt.style.use(&#39;fivethirtyeight&#39;) . . Populating the interactive namespace from numpy and matplotlib . @lru_cache(maxsize=512) def calculate_variance(totalhands: int, winrate: float = 0.0, dev: float = 100.0, samples: int = 1000): &quot;&quot;&quot; Another Poker Variance Calculator like Primedope&#39;s one Simulate randomly the amount of samples specified with winrate, totalhands and dev as parameters Display 20 samples, worst &amp; best samples, 95% and 68% confidence intervals, expected winnings, as well as risk of ruin up to 20% Print useful numbers about (un)expected variance Inputs: totalhands (int): number of hands e.g. desired length of the samples winrate (float): observed (or true) winrate we want to calculate the variance dev (float): standard deviation (&quot;Std Dev bb per 100 hands&quot; stat in HM2) sample (int): number of samples we want to simulate &quot;&quot;&quot; #limit number of samples if samples &gt; 1_000_000: samples = 1_000_000 start = time.time() h = totalhands//100 EV = winrate*h #standard error of the mean std_dev = dev/(h**0.5) #probability of loss with such a winrate and standard error prob_loss = stats.norm.cdf(0,loc=winrate,scale=std_dev) #x-axis x = np.arange(0, totalhands + 100, 100) #expected winnings y = [i * winrate for i in range(h+1)] #put ev values in a dataframe df = pd.DataFrame(y, columns=[&quot;ev&quot;], index=x) #confidence intervals df[&quot;conf1&quot;]= [i * (winrate + std_dev) for i in range(h+1)] df[&quot;conf2&quot;]= [i * (winrate - std_dev) for i in range(h+1)] df[&quot;conf3&quot;]= [i * (winrate + 2*std_dev) for i in range(h+1)] df[&quot;conf4&quot;]= [i * (winrate - 2*std_dev) for i in range(h+1)] #building ramdom samples with loc = winrate and scale = dev arr = np.zeros(shape=(samples,h+1)) for i in range(samples): arr[i,:]=np.cumsum(np.random.normal(winrate, dev, h+1)) #best and worst EV bottom_ten = np.argsort(arr[:,-1])[:10] top_ten = np.argsort(arr[:,-1])[-10:] #bottom line for each sample worst_downs = np.array([i for i in np.amin(arr, axis=1)]) #minimum bankroll required for each risk of ruin percentile #if the i-th percentile returns a positive value, minimum bankroll required is set to zero rr = range(0,101) min_bkr = [int(-np.percentile(worst_downs,i)) if np.percentile(worst_downs,i) &lt; 0 else 0 for i in rr] rr_df = pd.DataFrame({&quot;Risk of ruin&quot;: rr, &quot;Minimum Bankroll&quot;: min_bkr}) rr_df = rr_df.set_index(&quot;Risk of ruin&quot;) #best and worst samples df[&quot;best&quot;] = arr[top_ten[-1],:] df[&quot;worst&quot;] = arr[bottom_ten[-1],:] #print computing duration print(f&#39;Duration: {time.time() - start}s&#39;) #path to save image IMAGE_DIR = Path.cwd().parent / &quot;images&quot; plt.figure(figsize=(20,10)) #select randomly 20 samples to display excluding best and worst samples idx_array = [i for i in range(samples) if i not in [top_ten[-1],bottom_ten[-1]]] random_idx = np.random.choice(idx_array, 20, replace = False) for i in random_idx: plt.plot(x, arr[i,:],linewidth=1) #display confidence intervals as well as best and worst samples fmt=[&quot;b&quot;,&quot;g--&quot;,&quot;g--&quot;,&quot;g&quot;,&quot;g&quot;,&quot;c&quot;,&quot;y&quot;] labels=[&quot;EV&quot;,&quot;68% confidence interval&quot;,&quot;&quot;,&quot;95% confidence interval&quot;,&quot;&quot;,&quot;Best&quot;, &quot;Worst&quot;] for idx, col in enumerate(df.columns): df[col] = df[col].astype(int) plt.plot(df[col],fmt[idx],linewidth=3,label=labels[idx]) plt.title(&quot;Samples over %d hands with confidence intervals&quot; %totalhands) plt.xlabel(&quot;total hands&quot;) plt.ylabel(&quot;win/loss in big blinds&quot;) plt.legend(bbox_to_anchor=(0.0, 1.0), loc=&#39;upper left&#39;) plt.savefig(IMAGE_DIR / &quot;variance.png&quot;) plt.show() #some numbers to print ws = [winrate + std_dev, winrate - std_dev, winrate + (2*std_dev), winrate - (2*std_dev)] bb = [df.conf1.iloc[-1],df.conf2.iloc[-1],df.conf3.iloc[-1],df.conf4.iloc[-1]] std_dev_bb = int(std_dev*h) print (f&quot;Expected winnings: {winrate} bb/100 ({df.ev.iloc[-1]} bb)&quot;) print (f&quot;Standard deviation after {totalhands} hands: {std_dev: .2f} bb/100 ({std_dev_bb} bb)&quot;) print(&quot; n&quot;) print (f&quot;68% confidence interval: {ws[0]:.2f} bb/100 ({bb[0]} bb), {ws[1]:.2f} bb/100 ({bb[1]} bb)&quot;) print (f&quot;95% confidence interval: {ws[2]:.2f} bb/100 ({bb[2]} bb), {ws[3]:.2f} bb/100 ({bb[3]} bb)&quot;) print(&quot; n&quot;) print (f&quot;Top-10 over {samples} samples (bb/100): n{np.around(arr[top_ten][:,-1]/h, 2)}&quot;) print(&quot; n&quot;) print (f&quot;Bottom-10 over {samples} samples (bb/100): n{np.around(arr[bottom_ten][:,-1]/h, 2)}&quot;) print(&quot; n&quot;) print (f&quot;Probability of loss after {totalhands} hands: {prob_loss:.2%}&quot;) print(&quot; n&quot;) #display the first 20th percentiles of risk of ruin dataframe plt.figure(figsize=(20,10)) plt.plot(rr_df) plt.title(&quot;Risk of ruin n(q-th percentile over %s samples merged by their bottom line)&quot; %samples) plt.xlabel(&quot;risk of ruin %&quot;) plt.xlim(0,20) plt.xticks(range(0,22,2)) plt.ylabel(&quot;minimum bankroll in big blinds&quot;) plt.savefig(IMAGE_DIR / &quot;risk_of_ruin.png&quot;) plt.show() # return . . calculate_variance(totalhands = 80000, winrate = 5.5, dev = 100, samples=1000) . Duration: 0.06800365447998047s . Expected winnings: 5.5 bb/100 (4400 bb) Standard deviation after 80000 hands: 3.54 bb/100 (2828 bb) 68% confidence interval: 9.04 bb/100 (7228 bb), 1.96 bb/100 (1571 bb) 95% confidence interval: 12.57 bb/100 (10056 bb), -1.57 bb/100 (-1256 bb) Top-10 over 1000 samples (bb/100): [14.41 14.41 14.66 14.74 14.74 14.81 15.29 15.77 16.81 17.01] Bottom-10 over 1000 samples (bb/100): [-5.4 -5.02 -3.94 -3.3 -3.28 -3.11 -2.94 -2.84 -2.82 -2.72] Probability of loss after 80000 hands: 5.99% .",
            "url": "https://h7r.github.io/fast/notebook/jupyter/python/poker/2020/10/14/test-variance-calculator.html",
            "relUrl": "/notebook/jupyter/python/poker/2020/10/14/test-variance-calculator.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://h7r.github.io/fast/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://h7r.github.io/fast/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://h7r.github.io/fast/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}