{
  
    
        "post0": {
            "title": "Seasonality in Boston Index Crime rates",
            "content": ". from pathlib import Path #prevent from warnings from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates #%pylab inline plt.style.use(&#39;fivethirtyeight&#39;) plt.rcParams.update({&#39;font.size&#39;: 18}) . . &quot;Truth is not always in a well. In fact, as regards the more important knowledge, I do believe that she is invariably superficial. The depth lies in the valleys where we seek her, and not upon the mountain-tops where she is found.&quot; . &quot;By undue profundity we perplex and enfeeble thought; and it is possible to make even Venus herself vanish from the firmament by a scrutiny too sustained, too concentrated, or too direct.&quot; . &quot;It will be found, in fact, that the ingenious are always fanciful, and the truly imaginative never otherwise than analytic.&quot; . — Edgar Allan Poe, The Murders in the Rue Morgue (1841) . Be like a sniffer dog . Not a fun fact: in 2017, a violent crime occurred every 24.6 seconds according to the FBI Crime Clock [1]. This is the kind of statistics found &quot;upon the mountain-tops&quot;, by collecting mountains and mountains of daily crime data never ending. And the final result is reduced to this curious snapshot, like a nightmarish memento mori , namely the &quot;FBI Crime Clock&quot;: time is money, but danger lurks around every corner... A travel advisory issued by your embassy about a failed state would not have been more explicit. Do Not Travel! Reconsider! . No doubt indeed that it is possible to make enven the Six Grandfathers themselves vanish from the firmament of Mount Rushmore by such a sustained scrutinity into war on crime. Terrific decimal: 24.6. Better to never look at the watch, in fact. Nobody can breathe with such a number in mind. And there are a lot of records equally disturbing: 30.5 minutes (murders), 3.9 minutes (rapes), 1.7 minutes (robbery), etc. Ignorance of time is bliss. . Another quote from the same authority: . The 2017 violent crime rate was 382.9 per 100,000 inhabitants, down 0.9 percent when compared with the 2016 violent crime rate [2]. . How do you feel? Better, right? . Change the scale and you tell another story. . The truth that matters, the more important knowledge, says the detective Dupin-Poe, is invariably superficial, and lies in the valleys where we seek her. If not, we perplex and enfeeble thought. The Bostonian expert in solving enigmas is pro-Occam&#39;s razor. No need to undue profundity. Since the purloined letter is not hidden at all [3], why should I look in a well? &quot;Show the data!&quot;, warms Jeef Leek [4]. . So, this is a story of peaks and valleys, like in a Dow Jones chart. &quot;Pictures are undervalued in science&quot;, wrote Benoît Mandelbrot in his excellent book The (Mis)Behavior of Markets. We agree with the French mathematician. We believe that pictures, good pictures and not ridiculograms [5], are the royal road to epiphanies. . In this study, charts helped us a lot. Despite the noisy data, an unexpected pattern was showing up insistently from the very start:like temperature or influenza, crime is seasonal. Peak fever usually occurs in summer. Evidences pop up all over the data. Maybe it will be found, to paraphrase again Edgar Allan Poe, that data analysts are never otherwise than sniffer dogs following a promising track. . General setup . from pathlib import Path #prevent from warnings from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates plt.style.use(&#39;fivethirtyeight&#39;) plt.rcParams.update({&#39;font.size&#39;: 18}) . . Load and prepare the data . usecols=[0,3,7,10,12,14,15] index_col = &quot;OCCURRED_ON_DATE&quot; path = Path.cwd().parent / &#39;data&#39; / &#39;boston_2020-10-03.csv&#39; data=pd.read_csv(path, index_col=index_col, usecols=usecols, low_memory=False).sort_index() #rename columns col_names=[&quot;id&quot;,&quot;description&quot;,&quot;day_of_the_week&quot;,&quot;ucr&quot;,&quot;lat&quot;,&quot;long&quot;] to_rename=dict() for idx, col in enumerate(data.columns): to_rename[col]=col_names[idx] data=data.rename(columns=to_rename) #rename index data.index.rename(&quot;date&quot;,inplace=True) #set datetime type for index data.index = pd.to_datetime(pd.to_datetime(data.index).date) #print(data.index.dtype) #daily aggregate daily = data.index.value_counts(sort=False).sort_index() #print(daily.index.dtype) #remove last day for which report is incomplete daily=daily[0:len(daily)-1] #define some useful lists daily_index=daily.index months = [&#39;January&#39;, &#39;February&#39;, &#39;March&#39;, &#39;April&#39;, &#39;May&#39;, &#39;June&#39;, &#39;July&#39;, &#39;August&#39;, &#39;September&#39;, &#39;October&#39;, &#39;November&#39;, &#39;December&#39;] weekdays = [&quot;Monday&quot;,&quot;Tuesday&quot;,&quot;Wednesday&quot;,&quot;Thursday&quot;,&quot;Friday&quot;,&quot;Saturday&quot;,&quot;Sunday&quot;] . . Basic stuff . #print basic infos print(&quot;Start at: &quot; + str(data.index[0])) print(&quot;End at: &quot; + str(data.index[-1])) print(&quot;Number of days: &quot; + str(len(data.index.value_counts()))) print(&quot;Number of incidents (non uniques): &quot; + str(len(data))) #remove duplicated values data = data.drop_duplicates() #uncomment for sanity check #print(data.duplicated().sum()) #check if missing values. There are missing value only for the columns &quot;ucr&quot; &quot;lat&quot; and &quot;long&quot; but we don&#39;t need them #print(data.isnull().sum()) #print the top 30 offense types print(&quot; n&quot;) print(data[&quot;description&quot;].value_counts(ascending=False)[:30]) . . Start at: 2015-06-15 00:00:00 End at: 2020-10-03 00:00:00 Number of days: 1938 Number of incidents (non uniques): 518583 INVESTIGATE PERSON 32285 SICK/INJURED/MEDICAL - PERSON 29203 VERBAL DISPUTE 26418 M/V - LEAVING SCENE - PROPERTY DAMAGE 25831 VANDALISM 23370 ASSAULT SIMPLE - BATTERY 19876 INVESTIGATE PROPERTY 18742 TOWED MOTOR VEHICLE 17631 THREATS TO DO BODILY HARM 14646 LARCENY THEFT FROM MV - NON-ACCESSORY 13976 LARCENY THEFT FROM BUILDING 13963 LARCENY SHOPLIFTING 12668 PROPERTY - LOST 12233 WARRANT ARREST 10965 LARCENY ALL OTHERS 9913 M/V ACCIDENT - PROPERTY  DAMAGE 8901 M/V ACCIDENT - PERSONAL INJURY 7727 FRAUD - FALSE PRETENSE / SCHEME 7585 MISSING PERSON - LOCATED 7381 ASSAULT - SIMPLE 7112 ASSAULT - AGGRAVATED 6572 ASSAULT - AGGRAVATED - BATTERY 6522 PROPERTY - FOUND 5899 AUTO THEFT 5609 MISSING PERSON 5583 HARASSMENT 5557 M/V ACCIDENT - OTHER 5137 TRESPASSING 4782 FRAUD - CREDIT CARD / ATM FRAUD 4549 SICK ASSIST 4472 Name: description, dtype: int64 . Quick overview . . locator = mdates.MonthLocator(12) fmt = mdates.DateFormatter(&#39;%b. %Y&#39;) fig, ax = plt.subplots(2,2, figsize=(20, 15)) ax[0,0].hist(daily) ax[0,0].set_title(&quot;Daily incidents&quot;) ax[0,0].set_xlabel(&quot;# of incidents&quot;) ax[0,0].set_ylabel(&quot;# of days&quot;) ax[1,0].xaxis.set_major_locator(locator) ax[1,0].xaxis.set_major_formatter(fmt) ax[1,0].set_xlim(daily.index[0],daily.index[-1]) ax[1,0].plot(daily.index,daily) ax[1,0].set_ylabel(&quot;# of incidents&quot;) ax[0,1].hist(data[&quot;description&quot;].value_counts()) ax[0,1].set_xlabel(&quot;# of incidents&quot;) ax[0,1].set_ylabel(&quot;# of offense types&quot;) ax[0,1].set_title(&quot;Offense types&quot;) sum_counts = np.sum(data[&quot;description&quot;].value_counts()) cdf = data[&quot;description&quot;].value_counts(ascending=False).cumsum()[:30] ax[1,1].bar(range(30),cdf/sum_counts) ax[1,1].set_xlabel(&quot;# of offense types&quot;) ax[1,1].set_xticks(range(0,45,15)) ax[1,1].set_ylabel(&quot;cumulative distribution&quot;) plt.show() . . Fig. 1. — Crime Incident Reports overview. While the total number of incidents in Boston rarely exceeds 300 events on a single day (top left), following what looks like a perfect Gaussian distribution, the raw daily data is obscured by seasonal oscillations and a certain amount of stochasticity (bottom left). Note the sudden drop on bottom left: it occurs right after State of Emergency is declared on March 23, 2020. When the Reopening is announced on May 18, the number of daily incidents increases first slowly then quickly a few days after, hitting a local maximum on June 1. . First clues of seasonality and lockdown effect . Seasonality is a classic pattern in criminology though still without being well understood by academics. The subject has been studied for over a century, starting with Quetelet [6], Mister &quot;Average Man&quot;, who was an ingenuous believer, like almost all the scientists of his time (19th century), in &quot;natural&quot; or inborn biological causes behind deviance (climate and/or other stupid factors like race, heredity, etc.). In his (narrow) mind, statistics will save the world and the Average Man will replace the old imperfect man great-grandpa was. Here we are. More domestic disputes in summer, why? Maybe the sun making new highs removes some unpleasant ancestral barriers. As a result, the Average Man deviates from the boring statistical mean and the standard deviation of his expected human behavior turns crazy... Maybe. Quetelet: &quot;the violence of the passions predominating in summer, excites to more frequent personal collisions&quot;. In other no mother tongue terms: the passions predominating in his brain (whatever the season), excites Quetelet to very frequent confusion between correlation and causation. . More seriously, as mentioned by Dong et al. (2017) [7]: . Considering one of the most noticeable consequences of global warming is a shift in weather patterns, either locally warming or cooling, there is a need to understand how crime rates may change in response to the new weather patterns. . Less water in some countries, for example, and I bet that the Average Man will shift too. He will deviate to survive and become unpredictable. Welcome to the Extremistan Man. The extremistan studies are born. Bye-bye Quetelet and Mediocristan! Easy. . So this is not very surprising whether the the BPD Crime Incident Reports shows seasonal variations though obscured by noisy data (bottom left, Fig. 1). We need to build a tool to extract the long term trend. . If we look carefully at the same bottom panel on left in Fig. 1, a substantial hole is drawed at the end of the series. The phenomenon starts in fact shortly before the State of Emergency is declared on March 23, 2020 by Governor Baker, and ends after Reopening announcement [8] on May 18:this is what we call the lockdown effect (Fig. 2), i.e. an abnormal drop in the number of incidents due to the major part of the population being under strict control, empty streets and shops closed. After the economy reopen, the crime quickly reopened too, and climaxed on June 1 for then coming back to its old routine made of sudden peaks and drops – the seasonal oscillations. . We will show infra that the June 1 spike is most likely related to George Floyd protests maybe not so peaceful but running hot in response to police brutality. . From this point of view, crime – crimes against property more precisely – is nothing but a kind of thermometer. Take it (for analysis) or break it (with imprisonment and incapacitation effect), the thermometer says that burglary, auto-theft and vandalism have been the best riots indicators during George Floyd protests (Fig. 6). Rinse, repeat: the data shows that this people didn&#39;t target persons, but property, at least at Boston. This is why we need good data science. Because truth matters. Don&#39;t be afraid to tell the truth. She is often an outlier, a surprising and beautiful outlier. In other words, truth is a &quot;dramatic change&quot; in the data. The truth is that George Floyd died under the ruthless knee of a white police officer. . [...] social catastrophes can override any seasonality and lead to dramatic changes in future crime rates. (Dong et al., 2017) . That&#39;s it. . So, please, be careful when you remove outliers! . . #plot Covid lockdown window fig, ax = plt.subplots(figsize=(20, 10)) mn, mx = daily[&#39;2018-03-23&#39;:].min(), daily[&#39;2018-03-23&#39;:].max() ax.plot(daily[&#39;2018-03-23&#39;:]) ax.set_ylabel(&quot;# of incidents&quot;) #ax.set_xlim(&#39;2019-03-23&#39;,daily.index[-1]) ax.fill_between((&#39;2020-03-23&#39;,&#39;2020-05-18&#39;), mn, mx, facecolor=&#39;orange&#39;, alpha=0.2) plt.show() . . Fig. 2. — Zoom on lockdown effect. A significative drop in crime rate occurs during the lockdown at Boston (orange shape). The short term oscillations (seasonality) are not immune to brutal social changes. Wed&#39;ll show infra that the spike occurring on June 1 is due to George Floyd protests. . Another picture of the data. . A heatmap is a useful visualization technique to show in two dimensions the magnitude of a phenomenon with color variations. Since the Fig. 1 gives some clues of seasonality we decided to investigate, by reorganizing our daily time series into a 2d-array —x-axis for weekdays, y-axis for months—, the seasonal pattern if present should become evident once generated in the appropriate visual format. . . . plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(1,1, figsize=(15,5)) plt.yticks(range(7), weekdays) plt.xticks(range(12), months, rotation=&#39;vertical&#39;) ar = np.zeros(shape=(7,12),dtype=np.float32) for i in range(len(data)): d=data.index[i].weekday() m=data.index[i].month ar[d][m-1]+=1 img = ax.imshow(ar,plt.cm.Greens, interpolation=&#39;nearest&#39;) ax.grid(False) #ax.set_title(&quot;Fig. 3 - Daily incidents heatmap (by count)&quot;,fontsize=20) bins = np.linspace(ar.min(), ar.max(), 5) plt.colorbar(img, shrink=.6, ticks=bins) plt.show() . . Fig. 3. — Daily incidents heatmap (by count). The data have been reorganized in an array where each cell is the recipient of the total count of incidents occurred a given weekday of a given month of the year. The darkest zone arouns the middle of the figure indicates that the number of incidents is usually highest between June and September. . The color interpolation in Fig. 3 indicates a three/four-months range where incidents are reported at a highest frequency. For sure, it&#39;s a good rule of thumb to never draw conclusions too quickly from charts, but it&#39;s also true that they are very good food for thought. They help us a lot to navigate through the noisy data we try to understand better. So we would say this is a good start for us and hope future unexpected discoveries. . Next step, get a cleanest and narrowed data through feature selection. . Size doesn&#39;t matter: all we need is love and good data . By reducing our sample size and selecting carefully the offense types, our goal is to remove bias in the data collected by agents potentially involved in race for results (&quot;get those numbers and come back with them&quot;), which lead in the past to serious misconducts [13]. . The UCR Part I crimes [8], or Index Crimes, are divided in two categories: violent crimes (crime against persons) and property crimes (crime against property): . Violent Crimes Property Crimes . ASSAULT - AGGRAVATED | LARCENY | . MURDER | AUTO THEFT | . ROBBERY | ARSON | . RAPE | BURGLARY | . As said in introduction, arson offenses have been excluded from the Index Crimes due to inconsistent reporting. Due to inconsistency in the BPD dataset regarding sexual assault (change in law), we will also exclude the rapes from this study. . Here some functions we need: . def offenseExtractor(df, _name): &quot;&quot;&quot; Inputs: df: a dataframe _name: a string or a regular expression Output: a filtered dataframe which description column contains _name input &quot;&quot;&quot; return(df.loc[(df[&#39;description&#39;].str.contains(_name))]) def getIndexCrimes(data, _names): &quot;&quot;&quot; Input: data (DataFrame): the data we want to create features (offense types) from. _names: a list of strings or regular expressions. Output: DataFrame: A DataFrame with index crimes as columns. Note: use pre-declared list daily_index &quot;&quot;&quot; offenses = pd.DataFrame(index=daily_index) for _name in _names: offenses[_name] = aggregateBuilder(offenseExtractor(data, _name)) return offenses def aggregateBuilder(df): &quot;&quot;&quot; Inputs: df: a time series DataFrame Output: a daily aggregate (Series) Note: use pre-declared list daily_index &quot;&quot;&quot; #aggregate by day of the year and sort the index s = df.index.value_counts(sort=False).sort_index() #reindex with daily_index and fill the missing days with 0 as value s = s.reindex(daily_index, fill_value=0) return s def prepareHeatmap(df): #rename function? &quot;&quot;&quot; input: a time series output: a numpy array with shape(7,12) which x-axis = weekday and y-axis = month &quot;&quot;&quot; ar = np.zeros(shape=(7,12),dtype=np.float32) for i in range(len(df)): d=df.index[i].weekday() m=df.index[i].month ar[d][m-1]+=df.iloc[i] return ar . . keywords=[&quot;ASSAULT - AGGRAVATED&quot;, &quot;ROBBERY&quot;, &quot;MURDER&quot;, &quot;LARCENY&quot;, &quot;BURGLARY&quot;, &quot;AUTO THEFT&quot;] index_crimes = getIndexCrimes(data, keywords) . . index_crimes[&quot;violent_crimes&quot;]=index_crimes.iloc[:,0:3].sum(axis=1) index_crimes[&quot;property_crimes&quot;]=index_crimes.iloc[:,3:6].sum(axis=1) index_crimes[&quot;part_one_crimes&quot;]=index_crimes.iloc[:,6:8].sum(axis=1) #uncomment if you want to display the dataframe #display(index_crimes.head()) . . The Part I crimes as well as the property crimes heatmaps (Fig. 4) is like a reduced snapshot of the Fig. 3. . The same peak in (late) summer is still showing up despite we have strongly narrowed the data. It looks like the phenomena of seasonality is strong enough to be sample reduction resistant. . . fig = plt.figure(figsize = (15,10)) titles= [&quot;Part I Crimes&quot;, &quot;Violent Crimes&quot;, &quot;Property Crimes&quot;] #plt.yticks(range(21), weekdays*3) #plt.xticks(range(12), months, rotation=90) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) #plt.grid(False) #plt.set_xticklabels(range(12), months, rotation=&quot;vertical&quot;) #fig.suptitle(&quot;Fig. 3 - Most serious crimes&quot;, fontsize=30) #plt.label_outer() #plt.set_xticks(range(12), weekdays) for i in range(0,3): #img = ax[idx//2][0] if i == 0: df = prepareHeatmap(index_crimes.part_one_crimes) ax1.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax1.set_title(titles[i]) ax1.set_yticks(range(7)) ax1.set_yticklabels(weekdays) ax1.set_xticks(range(12)) ax1.set_xticklabels(months,rotation=90) ax1.grid(False) elif i== 1: df = prepareHeatmap(index_crimes.violent_crimes) ax2.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax2.set_title(titles[i]) ax2.set_yticks([]) #ax2.set_yticklabels(weekdays) ax2.set_xticks(range(12)) ax2.set_xticklabels(months,rotation=90) ax2.grid(False) else: df = prepareHeatmap(index_crimes.property_crimes) ax3.imshow(df,plt.cm.Greens,interpolation=&#39;nearest&#39;) ax3.set_title(titles[i]) ax3.set_yticks([]) #ax3.set_yticklabels(weekdays) ax3.set_xticks(range(12)) ax3.set_xticklabels(months,rotation=90) ax3.grid(False) #title=title[idx] + &quot; (&quot; + str(int(np.sum(df))) + &quot; obs.)&quot; #plt.title(title) #ax.set_xticklabels(months, rotation=&quot;vertical&quot;) #fig.delaxes(ax[4,1]) . . Fig. 4. — The most serious crimes by count. At first glance, Black Friday sales for crime shopping occurs in July. Spot seven differences between the left and the right heatmap is certainly a tough game. Sorry, no prize for the winner! It means that the weight of property crimes far exceeds the weight of violent crimes. . However, the violent crimes rate is not converging so well toward a clear seasonal peak, as if spotting the crimes against persons was in fact the real challenge here. . Is Boston doing better or worse than expected compared to other American cities? . By performing SSA, we will be able to answer the question in the next section. . The trajectory space of the crime . We start by quoting again Dong et al. (2017), our main source of inspiration for this work: . The first step in the SSA involves creating a trajectory matrix. For a time series $X = (x_1$, $x_2$,...,$x_N)$ and a window length $L = 365$, we define the trajectory matrix as: . $$ { bf X} = begin{pmatrix} x_1 &amp; x_2 &amp; x_3 &amp; cdots &amp; x_K x_2 &amp; x_3 &amp; x_4 &amp; cdots &amp; x_{K+1} x_3 &amp; x_4 &amp; x_5 &amp; cdots &amp; x_{K+2} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots x_L &amp; x_{L+1} &amp; x_{L+2} &amp; vdots &amp; x_N end{pmatrix} $$ with $K = N - L + 1$. Then a singular value decomposition (SVD) is performed on the trajectory matrix, providing a decomposition $X = X_1 + ⋯ + X_L$. Each of our modes then comes from averaging these matrices $X_i$ along their anti-diagonal. The mode with the largest singular value always corresponds to the average crime trend. We then typically would look for any modes with an annual periodicity in the first twenty singular values and sum these to produce our seasonal component. . Since we work under two constraints, time $t ne infty$ and budget $b approx0$, we don&#39;t go into detail. For those interested in learning more about SSA, see [9] and [10]. . Our purpose is to applying SSA technique to Boston following [4] guidelines. . from scipy.linalg import svd, diagsvd . . #creating a trajectory matrix def createLaggedVectors(s, L): &quot;&quot;&quot; inputs: s : series or 1d array-like L : an integer (the window length - long-term timeframe we want to extract) output: a 2d numpy array (Hankel matrix - all the elements along the diagonal are equal) &quot;&quot;&quot; N = len(s) K = N - L + 1 X = np.zeros(shape=(L,K),dtype=np.float64) for i in range (0,L): for j in range(0,K): X[i][j]+= s[j+i] return X def componentsExtractor(s,L): &quot;&quot;&quot; inputs: s : series or 1d array-like L : an integer (the window length - long-term timeframe we want to extract) output: a 2d numpy array of shape (len(s) x L) &quot;&quot;&quot; N = len(s) #trajectory matrix creation X = createLaggedVectors(s,L) #SVD decomposition avec scipy.linalg.svd U, s, VT = svd(X) #rank of the trajectory matrix, most of time d=L (no lineary dependence between columns of X) d = np.linalg.matrix_rank(X) #reconstruction X_components = np.zeros((N, L)) #thanks to Joan d&#39;Arcy Kaggle notebook [10] for the code below for i in range(0,d): #the tricky part: eigentriple grouping X_elem = s[i]*np.outer(U[:,i], VT[i,:]) X_rev = X_elem[::-1] #diagonal averaging X_components[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])] return X_components def checkAccuracy(s,ssa_array): &quot;&quot;&quot; Check if no error rebuilding the data time series from the eigentriples array inputs: s : series or 1d array-like ssa_array : 2d numpy array containing the eigentriples as columns output: True or False &quot;&quot;&quot; return np.allclose(s,np.sum(ssa_df,axis=1)) . . #uncomment for sanity check example #violent_crimes_ssa = componentsExtractor(index_crimes.violent_crimes, 365) # print(checkAccuracy(index_crimes.violent_crimes,violent_crimes_ssa)) . . #daily_ssa = componentsExtractor(daily, 365) part_one_crimes_ssa = componentsExtractor(index_crimes.part_one_crimes, 365) violent_crimes_ssa = componentsExtractor(index_crimes.violent_crimes, 365) property_crimes_ssa = componentsExtractor(index_crimes.property_crimes, 365) #separate trend component from seasonal component part_one_crimes_trend = pd.Series(part_one_crimes_ssa[:,0],daily_index) part_one_crimes_seasonal = pd.Series(part_one_crimes_ssa[:,0:3].sum(axis=1),daily_index) violent_crimes_trend = pd.Series(violent_crimes_ssa[:,0],daily_index) violent_crimes_seasonal = pd.Series(violent_crimes_ssa[:,0:3].sum(axis=1),daily_index) property_crimes_trend = pd.Series(property_crimes_ssa[:,0],daily_index) property_crimes_seasonal = pd.Series(property_crimes_ssa[:,0:3].sum(axis=1),daily_index) . . . titles= [&quot;Part I Crimes&quot;, &quot;Violent Crimes&quot;, &quot;Property Crimes&quot;] fig = plt.figure(figsize = (15,15)) ax1 = fig.add_subplot(311) ax2 = fig.add_subplot(312) ax3 = fig.add_subplot(313) #fig.suptitle(&quot;Fig. 4 - Most serious crimes with SSA components&quot;,fontsize=30) for i in range(0,3): if i == 0: ax1.plot(index_crimes.part_one_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax1.plot(part_one_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax1.plot(part_one_crimes_seasonal,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax1.set_title(titles[i]) ax1.set_xticks([]) ax1.legend() elif i== 1: ax2.plot(index_crimes.violent_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax2.plot(violent_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax2.plot(violent_crimes_seasonal ,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax2.set_title(titles[i]) ax2.set_xticks([]) ax2.set_ylabel(&quot;# of crimes&quot;, fontsize=20) else: ax3.plot(index_crimes.property_crimes,color=&quot;grey&quot;,label=&quot;raw data&quot;) ax3.plot(property_crimes_trend,color=&quot;orange&quot;,linewidth=2, label=&quot;trend&quot;) ax3.plot(property_crimes_seasonal ,color=&quot;green&quot;,linewidth=3,label=&quot;trend + seasonal&quot;) ax3.set_title(titles[i]) plt.show() . . Fig. 5. — The most serious crimes with their SSA components. Daily aggregates are plotted in grey, long term trends in yellow, and the green curves are the long term trends plus the first two seasonal components (ET1-3). See the big outlier on the right? . Recap: . SSA technique makes a decomposition of a time series $X$ of length $N$ (the diagonal of the trajectory matrix) into a sum of $L$ components of length $N$. To reconstruct the time series, simply sum the $L$ components and make a sanity check. | The mode with the largest values, i.e. the first elementary reconstructed series or first eigentriple (ET1), is the long term trend. | We sum the first other modes with annual periodicity to produce the seasonal component. | . Long term trends extraction (the signal) . The Index Crimes in Boston are slowly falling accordingly to the national tendency [11], but this is due above all to a sharp decline in property crimes frequency, while violent crimes long term trend is flat as a hell (Fig. 5). Due to a very low amplitude, annual variations for violent crimes would be almost impossible to detect without the SSA technique. . Back to the original dataset. Three offense types exhibe the same outlier (Fig. 6): . BURGLARY - COMMERICAL (sic) | AUTO THEFT | VANDALISM | . Quick research confirm that George Floyd protests in Boston streets on May 31 have ended in violence [12]. The many incidents have been reported by BDP one day later, on June 1, resulting in the outlier in the data. . We will now emphasize both violent crimes and property crimes long trends by zooming on their respective subcategories. . . plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;BURGLARY - COMMERICAL&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;BURGLARY - COMMERCIAL&quot;) plt.show() plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;AUTO THEFT&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;AUTO THEFT&quot;) plt.show() plt.figure(figsize=(10,3)) daily_offenses = aggregateBuilder(offenseExtractor(data,&quot;VANDALISM&quot;)) plt.plot(daily_offenses) plt.ylabel(&quot;# of crimes&quot;, fontsize=15) plt.title(&quot;VANDALISM&quot;) plt.show() . . Fig. 6. — Boston protests against George Floyd killing ending in violence. Commercial burglary, auto theft and vandalism exhibe the same spike on June 1, showing that these three offense types are the best markers for riots and looting. . Though we observe a general decrease for Part I Crimes (Fig. 5), the aggravated assaults are slightly on the rise and auto thefts have ceased to decrease after 2019, while the rate of decrease for burglary seems to be slowing (Fig. 7). As mentioned by [4]: . On possible cause is that crime is at very low basal level, and it is hard to decrease it below this level. . violent_crimes_list = [&#39;ASSAULT - AGGRAVATED&#39;, &#39;MURDER&#39;, &#39;ROBBERY&#39;] property_crimes_list = [&#39;LARCENY&#39;, &#39;AUTO THEFT&#39;, &#39;BURGLARY&#39;] years=[2015,2016,2017,2018,2019,2020,2021] . . . fig, ax = plt.subplots(2,1, figsize=(10,10)) ylabel = &quot;Trend $C$/$C_0$&quot; for i in range(0,2): #plt.suptitle(&quot;Fig. 5 - Long term trends by subcategories&quot;, fontsize=30) if i == 0: off_list = violent_crimes_list ax[0].set_xticks([]) title=&quot;Violent crimes trend&quot; else: off_list = property_crimes_list ax[1].set_xticklabels(years) title=&quot;Property crimes trend&quot; for idx, offense in enumerate(off_list): ssa = componentsExtractor(index_crimes[offense], 365)[:,0] trend = pd.Series(ssa, index=daily_index) #normalize the values s = (1+trend.pct_change()).cumprod().fillna(1) ax[i].plot(s) ax[i].set_ylabel(ylabel, fontsize=20) ax[i].set_ylim(0.4,1.4) ax[i].set_yticklabels([&quot;&quot;,0.6,0.8,1.,1.2,&quot;&quot;]) ax[i].set_title(title) ax[i].legend(off_list, loc=&quot;lower left&quot;) plt.show() . . &lt;ipython-input-22-8788afb0e16c&gt;:27: UserWarning: FixedFormatter should only be used together with FixedLocator ax[i].set_yticklabels([&#34;&#34;,0.6,0.8,1.,1.2,&#34;&#34;]) &lt;ipython-input-22-8788afb0e16c&gt;:14: UserWarning: FixedFormatter should only be used together with FixedLocator ax[1].set_xticklabels(years) . Fig. 7. — Long term trends by subcategories. Following Donet et al. (2017), the data has been normalized &quot;to be the fraction of the initial crime level for the first time point $C_0$&quot;. Murder trend is pretty erratic, but keep in mind that homicides occur very rarely (less than 1 by day in average), so the variance is higher. . This is a paradox: once reached very low levels of crime (proof of agents good work), it becomes harder to lower more the curve, and more likely to get bad results in the future (not a proof of agents bad work), losing therefore public funds. From this point of view, a public safety policy based only on quantified results is suboptimal and unfair to the agents, if not stressful for them (incentivizing the dangerous &quot;race for results&quot; pointed supra). . Seasonal components extraction (the noise) . To reconstruct the seasonal data, we will use the first $n$ leading SSA order modes, where $2 leq n leq4$, depending on the offense type, each one having different periodicity. . Again, we have followed the procedure described in [4]: . The trend is subtracted from the data and then the data is smoothed using a moving window average. As the oscillations are related to the total amount of crime, the data and seasonal component have been divided by the trend pointwise in time to normalize the oscillations. . Since our long term window length $L = 365$, we have smoothed the data with a monthly window average of length $l = 30$ before normalizing the oscillations. . . sorted_keywords = [&#39;ASSAULT - AGGRAVATED&#39;, &#39;LARCENY&#39;, &#39;BURGLARY&#39;,&#39;ROBBERY&#39;, &#39;AUTO THEFT&#39;] fig = plt.figure(figsize = (15,20)) #plt.yticks(range(21), weekdays*3) #plt.xticks(range(12), months, rotation=90) ax1 = fig.add_subplot(511) ax2 = fig.add_subplot(512) ax3 = fig.add_subplot(513) ax4 = fig.add_subplot(514) ax5 = fig.add_subplot(515) #plt.xticks(rotation=45) locator = mdates.MonthLocator(12) fmt = mdates.DateFormatter(&#39;%b. %Y&#39;) for i in range(0,5): daily_offenses = index_crimes[sorted_keywords[i]] title = sorted_keywords[i][0]+sorted_keywords[i][1:].lower() if i&lt;3: n=2 elif i==3: n=3 else: n=4 ssa = componentsExtractor(daily_offenses, 365)[:,:n+1] seasonal, trend = pd.Series(ssa[:,1:n+1].sum(axis=1), index=daily_index), pd.Series(ssa[:,0], index=daily_index) #put the seasonal component and the raw data on the same order smoothed_daily_offenses = (daily_offenses-trend).rolling(30,min_periods=1,center=True).mean() / trend seasonal = seasonal/trend if i== 0: ax1.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax1.plot(seasonal, label=&quot;Seasonal&quot;,color=&#39;orange&#39;) ax1.set_title(title) ax1.set_xticks([]) elif i== 1: ax2.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax2.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax2.set_title(title) ax2.set_xticks([]) elif i == 2: ax3.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax3.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax3.set_title(title) ax3.set_xticks([]) ax3.set_ylabel(&quot;Seasonal/Trend&quot;, fontsize=30) elif i == 3: ax4.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax4.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax4.set_title(title) ax4.set_xticks([]) else: ax5.plot(smoothed_daily_offenses, label=&quot;Raw data&quot;) ax5.plot(seasonal, label=&quot;Seasonal component&quot;,color=&#39;orange&#39;) ax5.set_title(title) ax5.xaxis.set_major_locator(locator) ax5.xaxis.set_major_formatter(fmt) ax5.set_xlim(smoothed_daily_offenses.index[0],smoothed_daily_offenses.index[-1]) . . Fig. 8. — Seasonal crime component. Even if the raw data has been smoothed, we see that the seasonal oscillations are fairly consistent, especially for aggravated assault, larceny and auto theft. In other words, each crime index type has a seasonal component. A large aberration appears for burglary during George Floyd protests, while a significant drop happens for robbery category during the lockdown, showing that robbery is the crime type that has been the most affected by State of Emergency. . . plt.figure(figsize=(10,5)) for i in range(0,5): daily_offenses = aggregateBuilder(offenseExtractor(data,sorted_keywords[i])) title = sorted_keywords[i][0]+sorted_keywords[i][1:].lower() if i&lt;3: n=2 elif i==3: n=3 else: n=4 ssa = componentsExtractor(daily_offenses, 365)[:,:n+1] seasonal, trend = pd.Series(ssa[:,1:n+1].sum(axis=1), index=daily_index), pd.Series(ssa[:,0], index=daily_index) plt.plot(seasonal, label=title) plt.legend(ncol=len(sorted_keywords), bbox_to_anchor=(0, 1), loc=&#39;lower left&#39;, fontsize=&#39;small&#39;) plt.show() . . Fig. 9. — Differences in seasonality.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; References . [1] Introduction to ridiculogram by M. E. J. Newman. Web. 2 July 2007. https://www.youtube.com/watch?v=YS-asmU3p_4 | [2] Analyse Boston - Crime Incident Reports (August 2015 - To Date) (Source: New System). (https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-new-system) | [3] USLegal - Index Crimes Law and Legal Definition. https://definitions.uslegal.com/i/index-crimes/ | [4] Dong, K., Cao, Y., Siercke, B., Wilber, M., &amp; McCalla, S. G. (2017). Advising caution in studying seasonal oscillations in crime rates. PloS one, 12(9), e0185432. https://doi.org/10.1371/journal.pone.0185432 | [5] Wikipedia - Singular spectrum analysis. https://en.wikipedia.org/wiki/Singular_spectrum_analysis | [6] Mass.gov - Reopening Massachusetts. May 18, 2020. [[PDF]](https://www.mass.gov/doc/reopening-massachusetts-may-18-2020/download) | [7] Wikipedia - Pareto principle. https://en.wikipedia.org/wiki/Pareto_principle | [8] FBI:UCR • Crimes in the U.S. 2010 • Offenses Definitions. https://ucr.fbi.gov/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/offense-definitions | [9] Golyandina, Nina &amp; Zhigljavsky, Anatoly. (2013). Singular Spectrum Analysis for Time Series. 10.1007/978-3-642-34913-3. https://www.researchgate.net/publication/260124592_Singular_Spectrum_Analysis_for_Time_Series | [10] Jordan D&#39;Arcy - Kaggle Notebook, Introducing SSA for Time Series Decomposition. https://www.kaggle.com/jdarcy/introducing-ssa-for-time-series-decomposition | [11] Brennan Center for Justice. Press Release. Crime Remains at Historic Lows in America. Web. June 12, 2018. https://www.brennancenter.org/our-work/analysis-opinion/crime-remains-historic-lows-america | [12] Boston Globe. By Jeremy C. Fox and John Hilliard. Boston protests against George Floyd killing begin peacefully, end in violence, arrests. Web. June 1, 2020. https://www.bostonglobe.com/2020/05/31/metro/three-protests-against-george-floyd-killing-planned-boston-sunday/ | [13] The New York Times. By Michelle Alexander. Why Police Lie Under Oath. Web. Feb. 2, 2013. https://www.nytimes.com/2013/02/03/opinion/sunday/why-police-officers-lie-under-oath.html?pagewanted=all&amp;_r=0 | . &lt;/div&gt; .",
            "url": "https://h7r.github.io/fast/notebook/jupyter/python/time%20series%20analysis/2022/01/16/boston-crime-report.html",
            "relUrl": "/notebook/jupyter/python/time%20series%20analysis/2022/01/16/boston-crime-report.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The (un)expected variance at poker",
            "content": "Table of Contents . . import time import numpy as np import pandas as pd import matplotlib.pyplot as plt from pathlib import Path from functools import lru_cache from scipy import stats %pylab inline plt.style.use(&#39;fivethirtyeight&#39;) . . Populating the interactive namespace from numpy and matplotlib . @lru_cache(maxsize=512) def calculate_variance(totalhands: int, winrate: float = 0.0, dev: float = 100.0, samples: int = 1000): &quot;&quot;&quot; Another Poker Variance Calculator like Primedope&#39;s one Simulate randomly the amount of samples specified with winrate, totalhands and dev as parameters Display 20 samples, worst &amp; best samples, 95% and 68% confidence intervals, expected winnings, as well as risk of ruin up to 20% Print useful numbers about (un)expected variance Inputs: totalhands (int): number of hands e.g. desired length of the samples winrate (float): observed (or true) winrate we want to calculate the variance dev (float): standard deviation (&quot;Std Dev bb per 100 hands&quot; stat in HM2) sample (int): number of samples we want to simulate &quot;&quot;&quot; #limit number of samples if samples &gt; 1_000_000: samples = 1_000_000 start = time.time() h = totalhands//100 EV = winrate*h #standard error of the mean std_dev = dev/(h**0.5) #probability of loss with such a winrate and standard error prob_loss = stats.norm.cdf(0,loc=winrate,scale=std_dev) #x-axis x = np.arange(0, totalhands + 100, 100) #expected winnings y = [i * winrate for i in range(h+1)] #put ev values in a dataframe df = pd.DataFrame(y, columns=[&quot;ev&quot;], index=x) #confidence intervals df[&quot;conf1&quot;]= [i * (winrate + std_dev) for i in range(h+1)] df[&quot;conf2&quot;]= [i * (winrate - std_dev) for i in range(h+1)] df[&quot;conf3&quot;]= [i * (winrate + 2*std_dev) for i in range(h+1)] df[&quot;conf4&quot;]= [i * (winrate - 2*std_dev) for i in range(h+1)] #building ramdom samples with loc = winrate and scale = dev arr = np.zeros(shape=(samples,h+1)) for i in range(samples): arr[i,:]=np.cumsum(np.random.normal(winrate, dev, h+1)) #best and worst EV bottom_ten = np.argsort(arr[:,-1])[:10] top_ten = np.argsort(arr[:,-1])[-10:] #bottom line for each sample worst_downs = np.array([i for i in np.amin(arr, axis=1)]) #minimum bankroll required for each risk of ruin percentile #if the i-th percentile returns a positive value, minimum bankroll required is set to zero rr = range(0,101) min_bkr = [int(-np.percentile(worst_downs,i)) if np.percentile(worst_downs,i) &lt; 0 else 0 for i in rr] rr_df = pd.DataFrame({&quot;Risk of ruin&quot;: rr, &quot;Minimum Bankroll&quot;: min_bkr}) rr_df = rr_df.set_index(&quot;Risk of ruin&quot;) #best and worst samples df[&quot;best&quot;] = arr[top_ten[-1],:] df[&quot;worst&quot;] = arr[bottom_ten[-1],:] #print computing duration print(f&#39;Duration: {time.time() - start}s&#39;) #path to save image IMAGE_DIR = Path.cwd().parent / &quot;images&quot; plt.figure(figsize=(20,10)) #select randomly 20 samples to display excluding best and worst samples idx_array = [i for i in range(samples) if i not in [top_ten[-1],bottom_ten[-1]]] random_idx = np.random.choice(idx_array, 20, replace = False) for i in random_idx: plt.plot(x, arr[i,:],linewidth=1) #display confidence intervals as well as best and worst samples fmt=[&quot;b&quot;,&quot;g--&quot;,&quot;g--&quot;,&quot;g&quot;,&quot;g&quot;,&quot;c&quot;,&quot;y&quot;] labels=[&quot;EV&quot;,&quot;68% confidence interval&quot;,&quot;&quot;,&quot;95% confidence interval&quot;,&quot;&quot;,&quot;Best&quot;, &quot;Worst&quot;] for idx, col in enumerate(df.columns): df[col] = df[col].astype(int) plt.plot(df[col],fmt[idx],linewidth=3,label=labels[idx]) plt.title(&quot;Samples over %d hands with confidence intervals&quot; %totalhands) plt.xlabel(&quot;total hands&quot;) plt.ylabel(&quot;win/loss in big blinds&quot;) plt.legend(bbox_to_anchor=(0.0, 1.0), loc=&#39;upper left&#39;) plt.savefig(IMAGE_DIR / &quot;variance.png&quot;) plt.show() #some numbers to print ws = [winrate + std_dev, winrate - std_dev, winrate + (2*std_dev), winrate - (2*std_dev)] bb = [df.conf1.iloc[-1],df.conf2.iloc[-1],df.conf3.iloc[-1],df.conf4.iloc[-1]] std_dev_bb = int(std_dev*h) print (f&quot;Expected winnings: {winrate} bb/100 ({df.ev.iloc[-1]} bb)&quot;) print (f&quot;Standard deviation after {totalhands} hands: {std_dev: .2f} bb/100 ({std_dev_bb} bb)&quot;) print(&quot; n&quot;) print (f&quot;68% confidence interval: {ws[0]:.2f} bb/100 ({bb[0]} bb), {ws[1]:.2f} bb/100 ({bb[1]} bb)&quot;) print (f&quot;95% confidence interval: {ws[2]:.2f} bb/100 ({bb[2]} bb), {ws[3]:.2f} bb/100 ({bb[3]} bb)&quot;) print(&quot; n&quot;) print (f&quot;Top-10 over {samples} samples (bb/100): n{np.around(arr[top_ten][:,-1]/h, 2)}&quot;) print(&quot; n&quot;) print (f&quot;Bottom-10 over {samples} samples (bb/100): n{np.around(arr[bottom_ten][:,-1]/h, 2)}&quot;) print(&quot; n&quot;) print (f&quot;Probability of loss after {totalhands} hands: {prob_loss:.2%}&quot;) print(&quot; n&quot;) #display the first 20th percentiles of risk of ruin dataframe plt.figure(figsize=(20,10)) plt.plot(rr_df) plt.title(&quot;Risk of ruin n(q-th percentile over %s samples merged by their bottom line)&quot; %samples) plt.xlabel(&quot;risk of ruin %&quot;) plt.xlim(0,20) plt.xticks(range(0,22,2)) plt.ylabel(&quot;minimum bankroll in big blinds&quot;) plt.savefig(IMAGE_DIR / &quot;risk_of_ruin.png&quot;) plt.show() # return . . calculate_variance(totalhands = 80000, winrate = 5.5, dev = 100, samples=1000) . Duration: 0.06800365447998047s . Expected winnings: 5.5 bb/100 (4400 bb) Standard deviation after 80000 hands: 3.54 bb/100 (2828 bb) 68% confidence interval: 9.04 bb/100 (7228 bb), 1.96 bb/100 (1571 bb) 95% confidence interval: 12.57 bb/100 (10056 bb), -1.57 bb/100 (-1256 bb) Top-10 over 1000 samples (bb/100): [14.41 14.41 14.66 14.74 14.74 14.81 15.29 15.77 16.81 17.01] Bottom-10 over 1000 samples (bb/100): [-5.4 -5.02 -3.94 -3.3 -3.28 -3.11 -2.94 -2.84 -2.82 -2.72] Probability of loss after 80000 hands: 5.99% .",
            "url": "https://h7r.github.io/fast/notebook/jupyter/python/poker/2020/10/14/test-variance-calculator.html",
            "relUrl": "/notebook/jupyter/python/poker/2020/10/14/test-variance-calculator.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://h7r.github.io/fast/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://h7r.github.io/fast/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://h7r.github.io/fast/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}